{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Learning PyTorch\n",
    "Here I will document my learning process of deep learning framework pytorch.\n",
    "- **Tensors** – the core data structure in PyTorch\n",
    "- **Autograd** – automatic differentiation for training\n",
    "- **Building Models with nn.Module** – how to define neural network architectures\n",
    "- **Common Layers & Activation Functions** – essential building blocks for networks\n",
    "- **Loss Functions** – how to quantify model performance\n",
    "- **Optimizers** – algorithms to update model parameters\n",
    "- **Training Loop Structure** – putting it all together to train a model\n",
    "- **Datasets & DataLoaders** – loading and batching data for training\n",
    "- **Using GPUs** – leveraging CUDA for faster computation\n",
    "- **Debugging & Common Pitfalls** – tips to avoid or fix common errors\n",
    "## Tensors\n",
    "**Tensors** are the fundamental data structure for storing and manipulating\n",
    "data. Tensor Attributes: Every tensor has a **shape** (telling you its dimensions), a **dtype** (data type, e.g. float32, int64), and a **device** (CPU or GPU) where it’s stored"
   ],
   "id": "d8892d70a22f46f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:45:16.479256Z",
     "start_time": "2026-01-06T22:45:12.887024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Directly from data (e.g. list or nested lists)\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data) # infers dtype automatically\n",
    "\n",
    "# 2. From a NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array) # shares memory with NumPy when possible\n",
    "\n",
    "# 3. Using built-in initializers\n",
    "x_ones = torch.ones_like(x_data) # tensor of ones with same shape as x_data\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float32) # random values, specifying dtype\n",
    "\n",
    "# 4. With specific shapes and values\n",
    "shape = (2, 3)\n",
    "rand_tensor = torch.rand(shape) # random values in [0,1)\n",
    "ones_tensor = torch.ones(shape) # all ones\n",
    "zeros_tensor = torch.zeros(shape) # all zeros\n",
    "\n",
    "tensor = torch.rand(3, 4)\n",
    "print(\"Shape:\", tensor.shape)\n",
    "print(\"Datatype:\", tensor.dtype)\n",
    "print(\"Device:\", tensor.device)"
   ],
   "id": "6b600028a88e89bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([3, 4])\n",
      "Datatype: torch.float32\n",
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:50:37.441102Z",
     "start_time": "2026-01-06T22:50:37.433358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic Operations\n",
    "\n",
    "# Indexing and slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "print(tensor[0]) # First row\n",
    "print(tensor[:, 0]) # First column\n",
    "\n",
    "# Elementwise operations\n",
    "tensor = torch.tensor([[1.0, 2.0],[3.0, 4.0]])\n",
    "tensor = tensor * 2 + 1\n",
    "print(tensor)\n",
    "\n",
    "# Matrix multiplication\n",
    "A = torch.rand(2, 3)\n",
    "B = torch.rand(3, 4)\n",
    "C = A @ B # matrix product resulting in shape (2,4)\n",
    "print(C)"
   ],
   "id": "2b91f0c4048c1828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([[3., 5.],\n",
      "        [7., 9.]])\n",
      "tensor([[1.7984, 1.0619, 1.2744, 0.6247],\n",
      "        [0.5072, 0.3412, 0.4018, 0.2094]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Automatic Differentiation (Autograd)\n",
    "Autograd frees you from manually computing gradients. It records operations on tensors to build a computational graph, and then it can backpropagate gradients through this graph for you. If you have a tensor that requires gradients `requires_grad=True`, PyTorch will track all operations on it. When you call `.backward()` , it computes the gradient of a scalar output with respect to all tensors that have `requires_grad=True` and contributed to that output. Those gradients are then stored in the `.grad` attribute of each tensor. By default, Tensors do not compute gradients. You need to explicitly indicate which tensors require grad. Model parameters (weights and biases in neural networks) are set to require gradients by default when using `nn.Module`.\n"
   ],
   "id": "1ad9d36042bfef82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T15:41:05.313926Z",
     "start_time": "2026-01-08T15:41:05.307798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a tensor and enable gradient tracking\n",
    "x = torch.tensor([3.0], requires_grad=True) # a tensor with value 3.0\n",
    "print(x.requires_grad) # True\n",
    "# Define a simple function of x\n",
    "y = x**2 + 2*x + 1 # y = x^2 + 2x + 1\n",
    "# Compute gradient dy/dx by backpropagation\n",
    "y.backward() # y is a scalar (1-element tensor), so we can call backward directly\n",
    "print(x.grad) # prints the gradient (dy/dx) at x=3.0 (2*x + 2)"
   ],
   "id": "fac1a31af8e7d185",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([8.])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T16:04:23.340620Z",
     "start_time": "2026-01-08T16:04:23.308173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "# data point\n",
    "X = torch.tensor([3.0])\n",
    "Y = torch.tensor([12.0])\n",
    "# Forward pass: compute prediction and loss\n",
    "pred = w * X + b # model prediction\n",
    "loss = (pred - Y)**2 # mean squared error (simplified for 1 point)\n",
    "# Backward pass: compute gradients\n",
    "loss.backward() # compute dloss/dw and dloss/db\n",
    "print(w.grad) # gradient of loss w.rt w\n",
    "print(b.grad) # gradient of loss w.rt b"
   ],
   "id": "5e6a426837955789",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-30.])\n",
      "tensor([-10.])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building Neural Networks with `nn.Module`\n",
    "An `nn.Module` is a base class for all neural network layers and models. Your custom models will also\n",
    "inherit from `nn.Module`. Modules provide a convenient way to encapsulate learnable parameters, layers, and the forward computation. When you subclass `nn.Module` , you can define layers as class attributes and implement a forward method defining how data flows through those layers. PyTorch will automatically collect your parameters and provide utility methods behind the scenes"
   ],
   "id": "628f3c81bd5bf91d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T16:14:32.844864Z",
     "start_time": "2026-01-08T16:14:32.809927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__() # initialize base class\n",
    "        # Define layers\n",
    "        self.hidden = nn.Linear(input_size, hidden_size) # linear layer 1\n",
    "        self.relu = nn.ReLU() # activation\n",
    "        self.output = nn.Linear(hidden_size, output_size) # linear layer 2 (output)\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x) # apply first linear layer\n",
    "        x = self.relu(x)   # apply ReLU activation\n",
    "        x = self.output(x) # apply second linear layer\n",
    "        return x\n"
   ],
   "id": "813eea97f2baf00e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In `__init__` , after calling `super().__init__()` , we define our layers: two `nn.Linear`\n",
    "layers and a `nn.ReLU` activation. Each of these is an nn.Module themselves. By assigning\n",
    "them to `self.hidden` , `self.relu`, etc., PyTorch registers them as sub-modules of our\n",
    "model. This means all their parameters (weights, biases) are now part of SimpleNet’s\n",
    "parameters.\n",
    "\n",
    "In the forward method, we define how the input tensor flows through the network: first\n",
    "through the hidden linear layer, then a ReLU, then the output linear layer. The output of the\n",
    "forward is the model’s output. Important: You don’t call the forward method manually;\n",
    "instead, you call the model instance on an input, like `model(x)` , which will internally invoke\n",
    "forward.\n",
    "\n",
    "**nn.Module and nested modules**: You can compose modules within modules. For example,\n",
    "`nn.Sequential` is a handy container module that you can use to stack layers in order without\n",
    "explicitly writing a forward method. In fact, our SimpleNet above could be alternatively written using `nn.Sequential`"
   ],
   "id": "260ea8292f034214"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T16:23:13.711405Z",
     "start_time": "2026-01-08T16:23:13.705391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = SimpleNet(10, 5, 2)\n",
    "print(model)\n"
   ],
   "id": "1fdeed8efe35c746",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Moving the model to a device**: You can call `model.to(device)` to move all its parameters to, say, the GPU (`if device=torch.device('cuda')`).\n",
    "\n",
    "## Common Layers and Activation Functions\n",
    "- **Linear (Fully Connected) Layer**: `nn.Linear(in_features, out_features)` – Applies a\n",
    "linear transformation. It has learnable weights of shape (out_features, in_features) and biases of shape (out_features). For example, `nn.Linear(28*28, 512)` would create a layer suitable for an input vector of size 784 (like a flattened 28×28 image) and produces an output of size 512\n",
    "- **Convolutional Layer**: `nn.Conv2d(in_channels, out_channels, kernel_size, ...)`. It will convolve learned kernels over input feature maps. There are analogous `nn.Conv1d` and `nn.Conv3d` for 1D (sequence) or 3D (volumetric) data. Convolutional layers are key for image recognition tasks\n",
    "- **Recurrent Layers**: `nn.LSTM`, `nn.GRU`, `nn.RNN` – Layers for sequence data (text, time series). These maintain internal state and are a bit more complex to use, but PyTorch’s implementations handle a lot of details for you. If you have a sequence of word embeddings, for example, an LSTM can process them and output a hidden state sequence.\n",
    "- **Embedding Layer**: `nn.Embedding(num_embeddings, embedding_dim)` – A trainable lookup table for discrete inputs (e.g. word indices to dense vectors). Used in NLP tasks to map token IDs to vectors.\n",
    "- **Dropout**: `nn.Dropout(p)` – Randomly zeroes out some fraction p of elements in the input (each forward pass) to help prevent overfitting. Dropout is only active in training mode (`model.train()`) and automatically de-activates in evaluation mode (`model.eval()`).\n",
    "- **BatchNorm**: `nn.BatchNorm1d`, `nn.BatchNorm2d` – Normalize the activations of the previous layer to have stable mean and variance, which can help training. Like Dropout, BatchNorm behaves differently in training vs. evaluation (during training it uses batch statistics, during eval it uses learned running stats).\n",
    "\n",
    "\n",
    "\n",
    "**Activation Functions**: Non-linear activations introduce the non-linearity needed for neural networks to learn complex patterns. PyTorch provides many common activations in `torch.nn` or in\n",
    "`torch.nn.functional`. Some widely used ones: - **ReLU**: `nn.ReLU()`. Very common default activation for hidden layers. **Sigmoid**: `nn.Sigmoid()`. Useful for probabilities in binary\n",
    "classification. - **Tanh**: `nn.Tanh()`. Used historically in some networks. **Softmax**: `nn.Softmax(dim)`. Often used for _multi-class_ output for interpretation, but note: for training classification models, you typically do not put a Softmax at the end if you’re using `nn.CrossEntropyLoss` (as that loss function expects raw logits and internally applies a LogSoftmax)\n",
    "\n",
    "## Loss Functions\n",
    "The goal of training is to minimize this loss. PyTorch provides many standard loss functions in torch.nn , typically as `nn.XLoss` classes (which can be used like functions).\n",
    "- **Mean Squared Error (MSE) Loss**: `nn.MSELoss` – Used for regression tasks (continuous output). It computes the average of squared differences between predicted and actual values.\n",
    "- **Binary Cross-Entropy Loss**: `nn.BCELoss` – Used for binary classification (often with a Sigmoid output). It calculates the binary cross-entropy between target and output probabilities. (There’s\n",
    "also `nn.BCEWithLogitsLoss` which is numerically more stable if you combine a sigmoid and binary entropy in one step.)\n",
    "- **Negative Log-Likelihood Loss**: `nn.NLLLoss` – Often used for classification in combination with a LogSoftmax output. It’s basically the negative log of the probability of the correct class.\n",
    "- **Cross-Entropy Loss**: `nn.CrossEntropyLoss` – The most commonly used loss for multi-class classification. It combines a softmax and NLLLoss in one single class. Important: If you use\n",
    "- **CrossEntropyLoss** , your model’s output should be raw, unnormalized scores (a.k.a. logits). `nn.CrossEntropyLoss` will internally apply F.log_softmax and compute NLLLoss . In\n",
    "other words, do not apply Softmax on your output before feeding it to CrossEntropyLoss , or you’ll be effectively applying softmax twice.\n",
    "\n",
    "## Optimizers and Updating Parameters\n",
    "The next step is to use those gradients to update the model’s parameters in order to reduce the loss. This is done by an optimizer. PyTorch provides many optimization algorithms in the torch.optim package. The most basic one is _Stochastic Gradient Descent_ (SGD), but there are others like _Adam_, _RMSprop_, _Adagrad_, etc.\n",
    "\n",
    "**Setting up an optimizer**: To create an optimizer, you need to specify which parameters it should\n",
    "update and what learning rate (step size) to use. Typically, you pass `model.parameters()` to the optimizer, which grabs all the learnable parameters of the model.\n"
   ],
   "id": "c2cc2be22adcfdd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T23:59:28.198371Z",
     "start_time": "2026-01-08T23:59:24.618439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "model = SimpleNet(10, 5, 2)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ],
   "id": "90d68503679a3cc",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. **Forward pass**: compute model predictions and loss.\n",
    "2. **Backward pass**: `loss.backward()` to compute gradients.\n",
    "3. **Update parameters**: `optimizer.step()` to adjust weights by gradients.\n",
    "4. **Zero gradients**: `optimizer.zero_grad()` to clear accumulated gradients for the next iteration.\n",
    "\n",
    "It’s **crucial** to zero the gradients after each update; otherwise, gradients from subsequent backward calls will accumulate (sum) on top of the current ones. Why do gradients accumulate? Because PyTorch, by default, adds the new gradient to any existing .grad value. This is useful for some algorithms (like accumulating gradients over multiple minibatches, or RNN TBPTT), but in standard training you want to update using the gradient of the current batch only. So you must zero out before computing the next batch’s gradient.\n",
    "\n",
    "**Learning Rate Schedulers**: PyTorch also provides schedulers (`torch.optim.lr_scheduler`) to\n",
    "adjust the learning rate during training (e.g., reduce it if validation loss plateaus). This can improve training.\n",
    "\n",
    "## Training Loop Structure\n",
    "1. Preparation: Decide number of epochs to train, ensure your model, optimizer, and data loader are ready. Optionally prepare a validation/test data loader for evaluation.\n",
    "2. Set the model to training mode: model.train() . This enables dropout and batchnorm to behave in training mode (if you use them).\n",
    "3. Loop over the training dataset:\n",
    "- For each batch, load the data (inputs and targets). If using a GPU, move them to the GPU device: `inputs, targets = inputs.to(device)`, `targets.to(device)` so that they match the model’s device.\n",
    "- Forward pass: outputs = model(inputs).\n",
    "- Compute loss: `loss = loss_fn(outputs, targets)`.\n",
    "- Backward pass: `optimizer.zero_grad()` (clear old gradients), then loss.backward() (compute new gradients).\n",
    "- Update params: `optimizer.step()` (adjust model weights).\n",
    "4. Evaluate on a validation set or test set to monitor performance: Set model to evaluation mode: `model.eval()`. This tells layers like dropout and batchnorm to fix their behavior (e.g., not dropping units, using running stats). Disable grad: `with torch.no_grad()`: – inside this block, loop over validation. DataLoader, compute outputs and loss/accuracy. Accumulate metrics. Print or record the validation loss/accuracy. You might also save the model checkpoint if it’s the best so far.\n",
    "5. Perhaps adjust learning rate if using a scheduler (optional).\n",
    "6. End of training: Save final model"
   ],
   "id": "d7687915ac0a7643"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T00:35:38.446669Z",
     "start_time": "2026-01-09T00:33:31.643058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                  # [0,255] -> [0,1], shape: (1,28,28)\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # standard MNIST normalization\n",
    "])\n",
    "\n",
    "full_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "test_ds = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train into train/val\n",
    "train_size = int(0.9 * len(full_train))\n",
    "val_size = len(full_train) - train_size\n",
    "train_ds, val_ds = random_split(full_train, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 28x28 -> 14x14\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 14x14 -> 7x7\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                 # (N,32,7,7) -> (N, 1568)\n",
    "            nn.Linear(32 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10)            # logits for 10 classes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  # expects logits + class indices\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "def run_eval(loader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def train_one_epoch(loader, model):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total += y.size(0)\n",
    "\n",
    "    return total_loss / total\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(train_loader, model)\n",
    "    val_loss, val_acc = run_eval(val_loader, model)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | train loss: {train_loss:.4f} | val loss: {val_loss:.4f} | val acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"mnist_smallcnn_best.pth\")\n",
    "\n",
    "print(f\"\\nBest val acc: {best_val_acc*100:.2f}%\")\n",
    "print(\"Saved best model to: mnist_smallcnn_best.pth\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"mnist_smallcnn_best.pth\", map_location=device))\n",
    "test_loss, test_acc = run_eval(test_loader, model)\n",
    "print(f\"Test loss: {test_loss:.4f} | Test acc: {test_acc*100:.2f}%\")"
   ],
   "id": "98893e35d5feb230",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "C:\\Users\\Veljko\\PycharmProjects\\pytorch\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/5 | train loss: 0.2420 | val loss: 0.0888 | val acc: 97.13%\n",
      "Epoch 02/5 | train loss: 0.0697 | val loss: 0.0549 | val acc: 98.45%\n",
      "Epoch 03/5 | train loss: 0.0493 | val loss: 0.0496 | val acc: 98.47%\n",
      "Epoch 04/5 | train loss: 0.0387 | val loss: 0.0411 | val acc: 98.82%\n",
      "Epoch 05/5 | train loss: 0.0323 | val loss: 0.0416 | val acc: 98.72%\n",
      "\n",
      "Best val acc: 98.82%\n",
      "Saved best model to: mnist_smallcnn_best.pth\n",
      "Test loss: 0.0318 | Test acc: 98.91%\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using GPUs for Acceleration\n",
    "**Moving tensors/models to GPU**: To use a GPU for computation, you must explicitly move your data to it. This includes model parameters and any tensors used in computations. Important: All inputs to a model must be on the same device as the model’s parameters. If your model is on GPU, and you pass a CPU tensor, you’ll get a runtime error: “_Expected all tensors to be on the same device_” . So always move your inputs (and targets) to the GPU as well.\n",
    "\n",
    "\n",
    "**Multiple GPUs**: PyTorch supports data parallelism (via `nn.DataParallel` or better, `DistributedDataParallel`). If you have one GPU, cuda:0 is the typical device. If multiple, you might specify device IDs or use DistributedDataParallel for best performance.\n",
    "\n",
    "\n",
    "By writing device = torch.device(\"cuda\" `if torch.cuda.is_available() else \"cpu\"`) and using `.to(device)` , your code will automatically work on CPU if no GPU is present. This is good practice to make your code portable.\n",
    "\n",
    "\n",
    "## Debugging and Common Pitfalls\n",
    "- Forgetting to switch between training/evaluation mode\n",
    "- Not zeroing gradients\n",
    "- Mixing up devices\n",
    "- Calling `model.forward()` instead of `model()`\n",
    "- Incorrect loss usage\n",
    "- Not using `torch.no_grad()` correctly\n",
    "- No improvement in training (model not learning)\n",
    "- Learning rate issues\n",
    "- Gradient issues\n",
    "- Overfitting a single batch"
   ],
   "id": "4274d4abb7570175"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
