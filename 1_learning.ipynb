{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Learning PyTorch\n",
    "Here I will document my learning process of deep learning framework pytorch.\n",
    "- **Tensors** – the core data structure in PyTorch\n",
    "- **Autograd** – automatic differentiation for training\n",
    "- **Building Models with nn.Module** – how to define neural network architectures\n",
    "- **Common Layers & Activation Functions** – essential building blocks for networks\n",
    "- **Loss Functions** – how to quantify model performance\n",
    "- **Optimizers** – algorithms to update model parameters\n",
    "- **Training Loop Structure** – putting it all together to train a model\n",
    "- **Datasets & DataLoaders** – loading and batching data for training\n",
    "- **Using GPUs** – leveraging CUDA for faster computation\n",
    "- **Debugging & Common Pitfalls** – tips to avoid or fix common errors\n",
    "## Tensors\n",
    "**Tensors** are the fundamental data structure for storing and manipulating\n",
    "data. Tensor Attributes: Every tensor has a **shape** (telling you its dimensions), a **dtype** (data type, e.g. float32, int64), and a **device** (CPU or GPU) where it’s stored"
   ],
   "id": "d8892d70a22f46f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:45:16.479256Z",
     "start_time": "2026-01-06T22:45:12.887024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Directly from data (e.g. list or nested lists)\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data) # infers dtype automatically\n",
    "\n",
    "# 2. From a NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array) # shares memory with NumPy when possible\n",
    "\n",
    "# 3. Using built-in initializers\n",
    "x_ones = torch.ones_like(x_data) # tensor of ones with same shape as x_data\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float32) # random values, specifying dtype\n",
    "\n",
    "# 4. With specific shapes and values\n",
    "shape = (2, 3)\n",
    "rand_tensor = torch.rand(shape) # random values in [0,1)\n",
    "ones_tensor = torch.ones(shape) # all ones\n",
    "zeros_tensor = torch.zeros(shape) # all zeros\n",
    "\n",
    "tensor = torch.rand(3, 4)\n",
    "print(\"Shape:\", tensor.shape)\n",
    "print(\"Datatype:\", tensor.dtype)\n",
    "print(\"Device:\", tensor.device)"
   ],
   "id": "6b600028a88e89bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([3, 4])\n",
      "Datatype: torch.float32\n",
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T22:50:37.441102Z",
     "start_time": "2026-01-06T22:50:37.433358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic Operations\n",
    "\n",
    "# Indexing and slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "print(tensor[0]) # First row\n",
    "print(tensor[:, 0]) # First column\n",
    "\n",
    "# Elementwise operations\n",
    "tensor = torch.tensor([[1.0, 2.0],[3.0, 4.0]])\n",
    "tensor = tensor * 2 + 1\n",
    "print(tensor)\n",
    "\n",
    "# Matrix multiplication\n",
    "A = torch.rand(2, 3)\n",
    "B = torch.rand(3, 4)\n",
    "C = A @ B # matrix product resulting in shape (2,4)\n",
    "print(C)"
   ],
   "id": "2b91f0c4048c1828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([[3., 5.],\n",
      "        [7., 9.]])\n",
      "tensor([[1.7984, 1.0619, 1.2744, 0.6247],\n",
      "        [0.5072, 0.3412, 0.4018, 0.2094]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Automatic Differentiation (Autograd)\n",
    "Autograd frees you from manually computing gradients. It records operations on tensors to build a computational graph, and then it can backpropagate gradients through this graph for you. If you have a tensor that requires gradients `requires_grad=True`, PyTorch will track all operations on it. When you call `.backward()` , it computes the gradient of a scalar output with respect to all tensors that have `requires_grad=True` and contributed to that output. Those gradients are then stored in the `.grad` attribute of each tensor. By default, Tensors do not compute gradients. You need to explicitly indicate which tensors require grad.\n"
   ],
   "id": "1ad9d36042bfef82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T23:11:39.216430Z",
     "start_time": "2026-01-06T23:11:39.210411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a tensor and enable gradient tracking\n",
    "x = torch.tensor([3.0], requires_grad=True) # a tensor with value 3.0\n",
    "print(x.requires_grad) # True\n",
    "# Define a simple function of x\n",
    "y = x**2 + 2*x + 1 # y = x^2 + 2x + 1\n",
    "# Compute gradient dy/dx by backpropagation\n",
    "y.backward() # y is a scalar (1-element tensor), so we can call backward directly\n",
    "print(x.grad) # prints the gradient (dy/dx) at x=3.0 (2*x + 2)"
   ],
   "id": "fac1a31af8e7d185",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([8.])\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
