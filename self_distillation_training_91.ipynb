{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J6LaKNZKNHFD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "wnSIc_XEkQUP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=256):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "        self.bn = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bn(self.conv(x))\n",
        "\n",
        "class SelfDistillResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        base = resnet18(pretrained=False)\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.layer1 = base.layer1 # 64\n",
        "        self.layer2 = base.layer2 # 128\n",
        "        self.layer3 = base.layer3 # 256\n",
        "        self.layer4 = base.layer4 # 512\n",
        "\n",
        "        # Bottlenecks\n",
        "        self.b1 = Bottleneck(64, 256)\n",
        "        self.b2 = Bottleneck(128, 256)\n",
        "        self.b3 = Bottleneck(256, 256)\n",
        "        self.b4 = Bottleneck(512, 256)\n",
        "\n",
        "        # Classifiers\n",
        "        self.fc1 = nn.Linear(256, num_classes)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.fc4 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "\n",
        "        f1 = self.layer1(x)\n",
        "        h1 = self.b1(f1)\n",
        "        p1 = self.fc1(F.adaptive_avg_pool2d(h1,1).flatten(1))\n",
        "\n",
        "        f2 = self.layer2(f1)\n",
        "        h2 = self.b2(f2)\n",
        "        p2 = self.fc2(F.adaptive_avg_pool2d(h2,1).flatten(1))\n",
        "\n",
        "        f3 = self.layer3(f2)\n",
        "        h3 = self.b3(f3)\n",
        "        p3 = self.fc3(F.adaptive_avg_pool2d(h3,1).flatten(1))\n",
        "\n",
        "        f4 = self.layer4(f3)\n",
        "        h4 = self.b4(f4)\n",
        "        p4 = self.fc4(F.adaptive_avg_pool2d(h4,1).flatten(1))\n",
        "\n",
        "        return [p1, p2, p3, p4], [h1, h2, h3, h4]"
      ],
      "metadata": {
        "id": "JYvY2CPMkUYr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CE = nn.CrossEntropyLoss()\n",
        "KL = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "MSE = nn.MSELoss()\n",
        "\n",
        "T = 4.0\n",
        "alpha = 0.7\n",
        "beta = 0.3\n",
        "\n",
        "def train_step(model, images, labels, optimizer):\n",
        "    logits, feats = model(images)\n",
        "\n",
        "    teacher_logits = logits[-1]\n",
        "    teacher_feat = feats[-1].detach()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(4):\n",
        "        student_logits = logits[i]\n",
        "        student_feat = feats[i]\n",
        "\n",
        "        loss_ce = CE(student_logits, labels)\n",
        "\n",
        "        if i == 3:\n",
        "            total_loss += loss_ce\n",
        "            continue\n",
        "\n",
        "        log_p = F.log_softmax(student_logits / T, dim=1)\n",
        "        q = F.softmax(teacher_logits / T, dim=1)\n",
        "        loss_kl = KL(log_p, q) * (T*T)\n",
        "\n",
        "        teacher_resized = F.interpolate(teacher_feat, size=student_feat.shape[2:], mode=\"bilinear\")\n",
        "        loss_l2 = MSE(student_feat, teacher_resized)\n",
        "\n",
        "        total_loss += loss_ce + alpha * loss_kl + beta * loss_l2\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return total_loss.item()"
      ],
      "metadata": {
        "id": "6VFqPTE7kY5R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.05\n",
        "epochs = 50\n",
        "model = SelfDistillResNet18().to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    model.train()\n",
        "    total_epoch_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        loss = train_step(model, images, labels, optimizer)\n",
        "        total_epoch_loss += loss\n",
        "\n",
        "    avg_loss = total_epoch_loss / len(trainloader)\n",
        "\n",
        "    scheduler.step()\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            logits, _ = model(images)\n",
        "            final_outputs = logits[-1]\n",
        "            _, predicted = torch.max(final_outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    acc = 100 * correct / total\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch+1} | Current lr: {current_lr:.4f} | Loss: {avg_loss:.4f} | Accuracy: {acc:.2f}%\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal training time: {total_time:.2f} seconds\")\n",
        "\n",
        "torch.save(model.state_dict(), \"saved_models/cifar_10.pt\")\n",
        "print(\"Model saved as saved_models/cifar_10.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gboEP6FLka-U",
        "outputId": "93e408df-6ec2-4a67-85bc-7dbdbc24e77a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Current lr: 0.0500 | Loss: 6.8106 | Accuracy: 52.40%\n",
            "Epoch 2 | Current lr: 0.0498 | Loss: 4.7885 | Accuracy: 62.03%\n",
            "Epoch 3 | Current lr: 0.0496 | Loss: 4.0486 | Accuracy: 68.93%\n",
            "Epoch 4 | Current lr: 0.0492 | Loss: 3.5853 | Accuracy: 63.47%\n",
            "Epoch 5 | Current lr: 0.0488 | Loss: 3.2461 | Accuracy: 65.46%\n",
            "Epoch 6 | Current lr: 0.0482 | Loss: 3.0227 | Accuracy: 75.48%\n",
            "Epoch 7 | Current lr: 0.0476 | Loss: 2.8167 | Accuracy: 76.88%\n",
            "Epoch 8 | Current lr: 0.0469 | Loss: 2.6839 | Accuracy: 79.67%\n",
            "Epoch 9 | Current lr: 0.0461 | Loss: 2.5615 | Accuracy: 68.75%\n",
            "Epoch 10 | Current lr: 0.0452 | Loss: 2.4708 | Accuracy: 80.16%\n",
            "Epoch 11 | Current lr: 0.0443 | Loss: 2.3898 | Accuracy: 80.42%\n",
            "Epoch 12 | Current lr: 0.0432 | Loss: 2.3036 | Accuracy: 81.29%\n",
            "Epoch 13 | Current lr: 0.0421 | Loss: 2.2411 | Accuracy: 76.83%\n",
            "Epoch 14 | Current lr: 0.0409 | Loss: 2.1804 | Accuracy: 81.09%\n",
            "Epoch 15 | Current lr: 0.0397 | Loss: 2.1125 | Accuracy: 80.46%\n",
            "Epoch 16 | Current lr: 0.0384 | Loss: 2.0609 | Accuracy: 81.14%\n",
            "Epoch 17 | Current lr: 0.0370 | Loss: 2.0226 | Accuracy: 79.49%\n",
            "Epoch 18 | Current lr: 0.0356 | Loss: 1.9708 | Accuracy: 81.66%\n",
            "Epoch 19 | Current lr: 0.0342 | Loss: 1.9154 | Accuracy: 82.00%\n",
            "Epoch 20 | Current lr: 0.0327 | Loss: 1.8806 | Accuracy: 81.23%\n",
            "Epoch 21 | Current lr: 0.0312 | Loss: 1.8208 | Accuracy: 79.30%\n",
            "Epoch 22 | Current lr: 0.0297 | Loss: 1.7899 | Accuracy: 82.54%\n",
            "Epoch 23 | Current lr: 0.0281 | Loss: 1.7220 | Accuracy: 82.78%\n",
            "Epoch 24 | Current lr: 0.0266 | Loss: 1.6884 | Accuracy: 84.11%\n",
            "Epoch 25 | Current lr: 0.0250 | Loss: 1.6525 | Accuracy: 79.97%\n",
            "Epoch 26 | Current lr: 0.0234 | Loss: 1.5969 | Accuracy: 84.51%\n",
            "Epoch 27 | Current lr: 0.0219 | Loss: 1.5630 | Accuracy: 84.69%\n",
            "Epoch 28 | Current lr: 0.0203 | Loss: 1.5160 | Accuracy: 85.66%\n",
            "Epoch 29 | Current lr: 0.0188 | Loss: 1.4542 | Accuracy: 86.76%\n",
            "Epoch 30 | Current lr: 0.0173 | Loss: 1.4154 | Accuracy: 87.93%\n",
            "Epoch 31 | Current lr: 0.0158 | Loss: 1.3752 | Accuracy: 86.42%\n",
            "Epoch 32 | Current lr: 0.0144 | Loss: 1.3330 | Accuracy: 87.85%\n",
            "Epoch 33 | Current lr: 0.0130 | Loss: 1.2699 | Accuracy: 88.46%\n",
            "Epoch 34 | Current lr: 0.0116 | Loss: 1.2192 | Accuracy: 87.04%\n",
            "Epoch 35 | Current lr: 0.0103 | Loss: 1.1737 | Accuracy: 88.03%\n",
            "Epoch 36 | Current lr: 0.0091 | Loss: 1.1349 | Accuracy: 88.56%\n",
            "Epoch 37 | Current lr: 0.0079 | Loss: 1.0791 | Accuracy: 89.85%\n",
            "Epoch 38 | Current lr: 0.0068 | Loss: 1.0179 | Accuracy: 89.91%\n",
            "Epoch 39 | Current lr: 0.0057 | Loss: 0.9696 | Accuracy: 90.18%\n",
            "Epoch 40 | Current lr: 0.0048 | Loss: 0.9230 | Accuracy: 90.19%\n",
            "Epoch 41 | Current lr: 0.0039 | Loss: 0.8894 | Accuracy: 90.45%\n",
            "Epoch 42 | Current lr: 0.0031 | Loss: 0.8439 | Accuracy: 90.71%\n",
            "Epoch 43 | Current lr: 0.0024 | Loss: 0.8164 | Accuracy: 90.90%\n",
            "Epoch 44 | Current lr: 0.0018 | Loss: 0.7969 | Accuracy: 91.11%\n",
            "Epoch 45 | Current lr: 0.0012 | Loss: 0.7673 | Accuracy: 91.15%\n",
            "Epoch 46 | Current lr: 0.0008 | Loss: 0.7491 | Accuracy: 91.22%\n",
            "Epoch 47 | Current lr: 0.0004 | Loss: 0.7309 | Accuracy: 91.06%\n",
            "Epoch 48 | Current lr: 0.0002 | Loss: 0.7252 | Accuracy: 91.26%\n",
            "Epoch 49 | Current lr: 0.0000 | Loss: 0.7170 | Accuracy: 91.37%\n",
            "Epoch 50 | Current lr: 0.0000 | Loss: 0.7096 | Accuracy: 91.69%\n",
            "\n",
            "Total training time: 3466.32 seconds\n",
            "Model saved as saved_models/cifar_10.pt\n"
          ]
        }
      ]
    }
  ]
}