{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6LaKNZKNHFD",
        "outputId": "715c639e-4a7d-40ea-d554-65ff9f58c2bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet50\n",
        "import time\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "    transforms.RandomErasing(p=0.5)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset,\n",
        "    batch_size=100,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")"
      ],
      "metadata": {
        "id": "wnSIc_XEkQUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95f8e54-c587-4567-9697-c5408991de5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 42.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet50(num_classes=10).to(device)\n",
        "lr = 0.1\n",
        "epochs = 300\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "start_time = time.time()\n",
        "\n",
        "best_acc = 0\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in trainloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    acc = 100. * correct / total\n",
        "    print(f'Epoch {epoch+1} | Current lr: {current_lr:.4f} | Loss: {running_loss/len(trainloader):.4f} | Acc: {acc:.2f}%')\n",
        "\n",
        "    if acc > best_acc:\n",
        "        torch.save(model.state_dict(), 'saved_models/teacher_resnet50.pt')\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/resnet50_cifar10.pt\")\n",
        "        best_acc = acc\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal training time: {total_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYvY2CPMkUYr",
        "outputId": "c330d0a9-916c-40a1-bc47-cafa390c24ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Current lr: 0.1000 | Loss: 5.8009 | Acc: 9.55%\n",
            "Epoch 2 | Current lr: 0.1000 | Loss: 2.5221 | Acc: 16.23%\n",
            "Epoch 3 | Current lr: 0.1000 | Loss: 2.1594 | Acc: 27.92%\n",
            "Epoch 4 | Current lr: 0.1000 | Loss: 1.9951 | Acc: 28.43%\n",
            "Epoch 5 | Current lr: 0.0999 | Loss: 1.8886 | Acc: 35.98%\n",
            "Epoch 6 | Current lr: 0.0999 | Loss: 1.8062 | Acc: 38.75%\n",
            "Epoch 7 | Current lr: 0.0999 | Loss: 1.7439 | Acc: 41.38%\n",
            "Epoch 8 | Current lr: 0.0998 | Loss: 1.6798 | Acc: 42.69%\n",
            "Epoch 9 | Current lr: 0.0998 | Loss: 1.6346 | Acc: 45.06%\n",
            "Epoch 10 | Current lr: 0.0997 | Loss: 1.5792 | Acc: 46.46%\n",
            "Epoch 11 | Current lr: 0.0997 | Loss: 1.5290 | Acc: 49.60%\n",
            "Epoch 12 | Current lr: 0.0996 | Loss: 1.4830 | Acc: 52.59%\n",
            "Epoch 13 | Current lr: 0.0995 | Loss: 1.4374 | Acc: 54.96%\n",
            "Epoch 14 | Current lr: 0.0995 | Loss: 1.4045 | Acc: 55.51%\n",
            "Epoch 15 | Current lr: 0.0994 | Loss: 1.3641 | Acc: 58.66%\n",
            "Epoch 16 | Current lr: 0.0993 | Loss: 1.3324 | Acc: 58.77%\n",
            "Epoch 17 | Current lr: 0.0992 | Loss: 1.2961 | Acc: 60.28%\n",
            "Epoch 18 | Current lr: 0.0991 | Loss: 1.2684 | Acc: 60.02%\n",
            "Epoch 19 | Current lr: 0.0990 | Loss: 1.2462 | Acc: 61.54%\n",
            "Epoch 20 | Current lr: 0.0989 | Loss: 1.2193 | Acc: 62.13%\n",
            "Epoch 21 | Current lr: 0.0988 | Loss: 1.2078 | Acc: 64.41%\n",
            "Epoch 22 | Current lr: 0.0987 | Loss: 1.1914 | Acc: 63.95%\n",
            "Epoch 23 | Current lr: 0.0986 | Loss: 1.1596 | Acc: 63.83%\n",
            "Epoch 24 | Current lr: 0.0984 | Loss: 1.1338 | Acc: 62.92%\n",
            "Epoch 25 | Current lr: 0.0983 | Loss: 1.1137 | Acc: 65.39%\n",
            "Epoch 26 | Current lr: 0.0982 | Loss: 1.0921 | Acc: 63.99%\n",
            "Epoch 27 | Current lr: 0.0980 | Loss: 1.0664 | Acc: 65.68%\n",
            "Epoch 28 | Current lr: 0.0979 | Loss: 1.0546 | Acc: 67.47%\n",
            "Epoch 29 | Current lr: 0.0977 | Loss: 1.0345 | Acc: 70.24%\n",
            "Epoch 30 | Current lr: 0.0976 | Loss: 1.0173 | Acc: 70.01%\n",
            "Epoch 31 | Current lr: 0.0974 | Loss: 1.0042 | Acc: 69.79%\n",
            "Epoch 32 | Current lr: 0.0972 | Loss: 0.9891 | Acc: 69.75%\n",
            "Epoch 33 | Current lr: 0.0970 | Loss: 0.9724 | Acc: 68.42%\n",
            "Epoch 34 | Current lr: 0.0969 | Loss: 0.9733 | Acc: 70.75%\n",
            "Epoch 35 | Current lr: 0.0967 | Loss: 0.9506 | Acc: 68.53%\n",
            "Epoch 36 | Current lr: 0.0965 | Loss: 0.9561 | Acc: 72.90%\n",
            "Epoch 37 | Current lr: 0.0963 | Loss: 0.9339 | Acc: 71.03%\n",
            "Epoch 38 | Current lr: 0.0961 | Loss: 0.9350 | Acc: 72.43%\n",
            "Epoch 39 | Current lr: 0.0959 | Loss: 0.9233 | Acc: 74.09%\n",
            "Epoch 40 | Current lr: 0.0957 | Loss: 0.9206 | Acc: 73.06%\n",
            "Epoch 41 | Current lr: 0.0955 | Loss: 0.9080 | Acc: 73.76%\n",
            "Epoch 42 | Current lr: 0.0952 | Loss: 0.9013 | Acc: 70.89%\n",
            "Epoch 43 | Current lr: 0.0950 | Loss: 0.8939 | Acc: 74.64%\n",
            "Epoch 44 | Current lr: 0.0948 | Loss: 0.8968 | Acc: 74.47%\n",
            "Epoch 45 | Current lr: 0.0946 | Loss: 0.8831 | Acc: 74.75%\n",
            "Epoch 46 | Current lr: 0.0943 | Loss: 0.8753 | Acc: 75.55%\n",
            "Epoch 47 | Current lr: 0.0941 | Loss: 0.8746 | Acc: 74.27%\n",
            "Epoch 48 | Current lr: 0.0938 | Loss: 0.8777 | Acc: 73.74%\n",
            "Epoch 49 | Current lr: 0.0936 | Loss: 0.8746 | Acc: 74.56%\n",
            "Epoch 50 | Current lr: 0.0933 | Loss: 0.8555 | Acc: 76.71%\n",
            "Epoch 51 | Current lr: 0.0930 | Loss: 0.8549 | Acc: 71.00%\n",
            "Epoch 52 | Current lr: 0.0928 | Loss: 0.8500 | Acc: 75.14%\n",
            "Epoch 53 | Current lr: 0.0925 | Loss: 0.8454 | Acc: 74.66%\n",
            "Epoch 54 | Current lr: 0.0922 | Loss: 0.8461 | Acc: 73.72%\n",
            "Epoch 55 | Current lr: 0.0919 | Loss: 0.8385 | Acc: 74.97%\n",
            "Epoch 56 | Current lr: 0.0916 | Loss: 0.8379 | Acc: 74.36%\n",
            "Epoch 57 | Current lr: 0.0914 | Loss: 0.8377 | Acc: 75.23%\n",
            "Epoch 58 | Current lr: 0.0911 | Loss: 0.8281 | Acc: 75.11%\n",
            "Epoch 59 | Current lr: 0.0908 | Loss: 0.8252 | Acc: 76.81%\n",
            "Epoch 60 | Current lr: 0.0905 | Loss: 0.8186 | Acc: 74.64%\n",
            "Epoch 61 | Current lr: 0.0901 | Loss: 0.8145 | Acc: 71.16%\n",
            "Epoch 62 | Current lr: 0.0898 | Loss: 0.8119 | Acc: 77.70%\n",
            "Epoch 63 | Current lr: 0.0895 | Loss: 0.8089 | Acc: 75.32%\n",
            "Epoch 64 | Current lr: 0.0892 | Loss: 0.8042 | Acc: 76.62%\n",
            "Epoch 65 | Current lr: 0.0889 | Loss: 0.7985 | Acc: 73.56%\n",
            "Epoch 66 | Current lr: 0.0885 | Loss: 0.8046 | Acc: 75.66%\n",
            "Epoch 67 | Current lr: 0.0882 | Loss: 0.8006 | Acc: 76.76%\n",
            "Epoch 68 | Current lr: 0.0878 | Loss: 0.7904 | Acc: 76.60%\n",
            "Epoch 69 | Current lr: 0.0875 | Loss: 0.7922 | Acc: 78.34%\n",
            "Epoch 70 | Current lr: 0.0872 | Loss: 0.7812 | Acc: 77.87%\n",
            "Epoch 71 | Current lr: 0.0868 | Loss: 0.7871 | Acc: 72.29%\n",
            "Epoch 72 | Current lr: 0.0864 | Loss: 0.7820 | Acc: 78.39%\n",
            "Epoch 73 | Current lr: 0.0861 | Loss: 0.7852 | Acc: 76.18%\n",
            "Epoch 74 | Current lr: 0.0857 | Loss: 0.7727 | Acc: 76.98%\n",
            "Epoch 75 | Current lr: 0.0854 | Loss: 0.7738 | Acc: 74.31%\n",
            "Epoch 76 | Current lr: 0.0850 | Loss: 0.7804 | Acc: 77.94%\n",
            "Epoch 77 | Current lr: 0.0846 | Loss: 0.7664 | Acc: 78.14%\n",
            "Epoch 78 | Current lr: 0.0842 | Loss: 0.7638 | Acc: 77.93%\n",
            "Epoch 79 | Current lr: 0.0838 | Loss: 0.7637 | Acc: 77.45%\n",
            "Epoch 80 | Current lr: 0.0835 | Loss: 0.7581 | Acc: 77.40%\n",
            "Epoch 81 | Current lr: 0.0831 | Loss: 0.7586 | Acc: 75.29%\n",
            "Epoch 82 | Current lr: 0.0827 | Loss: 0.7550 | Acc: 78.98%\n",
            "Epoch 83 | Current lr: 0.0823 | Loss: 0.7524 | Acc: 74.29%\n",
            "Epoch 84 | Current lr: 0.0819 | Loss: 0.7535 | Acc: 76.30%\n",
            "Epoch 85 | Current lr: 0.0815 | Loss: 0.7513 | Acc: 76.58%\n",
            "Epoch 86 | Current lr: 0.0811 | Loss: 0.7440 | Acc: 76.38%\n",
            "Epoch 87 | Current lr: 0.0806 | Loss: 0.7343 | Acc: 78.41%\n",
            "Epoch 88 | Current lr: 0.0802 | Loss: 0.7384 | Acc: 77.81%\n",
            "Epoch 89 | Current lr: 0.0798 | Loss: 0.7442 | Acc: 76.57%\n",
            "Epoch 90 | Current lr: 0.0794 | Loss: 0.7352 | Acc: 77.34%\n",
            "Epoch 91 | Current lr: 0.0790 | Loss: 0.7319 | Acc: 79.84%\n",
            "Epoch 92 | Current lr: 0.0785 | Loss: 0.7313 | Acc: 78.42%\n",
            "Epoch 93 | Current lr: 0.0781 | Loss: 0.7375 | Acc: 79.56%\n",
            "Epoch 94 | Current lr: 0.0777 | Loss: 0.7254 | Acc: 76.85%\n",
            "Epoch 95 | Current lr: 0.0772 | Loss: 0.7213 | Acc: 77.58%\n",
            "Epoch 96 | Current lr: 0.0768 | Loss: 0.7187 | Acc: 80.17%\n",
            "Epoch 97 | Current lr: 0.0763 | Loss: 0.7290 | Acc: 79.36%\n",
            "Epoch 98 | Current lr: 0.0759 | Loss: 0.7147 | Acc: 79.73%\n",
            "Epoch 99 | Current lr: 0.0755 | Loss: 0.7188 | Acc: 78.42%\n",
            "Epoch 100 | Current lr: 0.0750 | Loss: 0.7175 | Acc: 78.03%\n",
            "Epoch 101 | Current lr: 0.0745 | Loss: 0.7149 | Acc: 77.93%\n",
            "Epoch 102 | Current lr: 0.0741 | Loss: 0.7111 | Acc: 78.11%\n",
            "Epoch 103 | Current lr: 0.0736 | Loss: 0.7036 | Acc: 77.63%\n",
            "Epoch 104 | Current lr: 0.0732 | Loss: 0.7110 | Acc: 79.98%\n",
            "Epoch 105 | Current lr: 0.0727 | Loss: 0.6992 | Acc: 80.30%\n",
            "Epoch 106 | Current lr: 0.0722 | Loss: 0.6978 | Acc: 78.53%\n",
            "Epoch 107 | Current lr: 0.0718 | Loss: 0.6939 | Acc: 81.00%\n",
            "Epoch 108 | Current lr: 0.0713 | Loss: 0.6981 | Acc: 80.29%\n",
            "Epoch 109 | Current lr: 0.0708 | Loss: 0.6883 | Acc: 76.28%\n",
            "Epoch 110 | Current lr: 0.0703 | Loss: 0.6921 | Acc: 78.74%\n",
            "Epoch 111 | Current lr: 0.0699 | Loss: 0.6894 | Acc: 77.84%\n",
            "Epoch 112 | Current lr: 0.0694 | Loss: 0.6921 | Acc: 80.13%\n",
            "Epoch 113 | Current lr: 0.0689 | Loss: 0.6797 | Acc: 79.27%\n",
            "Epoch 114 | Current lr: 0.0684 | Loss: 0.6882 | Acc: 79.38%\n",
            "Epoch 115 | Current lr: 0.0679 | Loss: 0.6762 | Acc: 76.76%\n",
            "Epoch 116 | Current lr: 0.0674 | Loss: 0.6832 | Acc: 81.20%\n",
            "Epoch 117 | Current lr: 0.0669 | Loss: 0.6844 | Acc: 81.41%\n",
            "Epoch 118 | Current lr: 0.0664 | Loss: 0.6792 | Acc: 80.35%\n",
            "Epoch 119 | Current lr: 0.0659 | Loss: 0.6730 | Acc: 79.90%\n",
            "Epoch 120 | Current lr: 0.0655 | Loss: 0.6704 | Acc: 79.23%\n",
            "Epoch 121 | Current lr: 0.0650 | Loss: 0.6722 | Acc: 80.72%\n",
            "Epoch 122 | Current lr: 0.0645 | Loss: 0.6693 | Acc: 80.23%\n",
            "Epoch 123 | Current lr: 0.0639 | Loss: 0.6690 | Acc: 79.62%\n",
            "Epoch 124 | Current lr: 0.0634 | Loss: 0.6582 | Acc: 78.38%\n",
            "Epoch 125 | Current lr: 0.0629 | Loss: 0.6556 | Acc: 81.94%\n",
            "Epoch 126 | Current lr: 0.0624 | Loss: 0.6565 | Acc: 80.08%\n",
            "Epoch 127 | Current lr: 0.0619 | Loss: 0.6584 | Acc: 80.75%\n",
            "Epoch 128 | Current lr: 0.0614 | Loss: 0.6512 | Acc: 82.09%\n",
            "Epoch 129 | Current lr: 0.0609 | Loss: 0.6509 | Acc: 79.67%\n",
            "Epoch 130 | Current lr: 0.0604 | Loss: 0.6460 | Acc: 80.19%\n",
            "Epoch 131 | Current lr: 0.0599 | Loss: 0.6471 | Acc: 80.48%\n",
            "Epoch 132 | Current lr: 0.0594 | Loss: 0.6476 | Acc: 80.06%\n",
            "Epoch 133 | Current lr: 0.0589 | Loss: 0.6387 | Acc: 80.95%\n",
            "Epoch 134 | Current lr: 0.0583 | Loss: 0.6416 | Acc: 81.38%\n",
            "Epoch 135 | Current lr: 0.0578 | Loss: 0.6342 | Acc: 80.91%\n",
            "Epoch 136 | Current lr: 0.0573 | Loss: 0.6320 | Acc: 81.57%\n",
            "Epoch 137 | Current lr: 0.0568 | Loss: 0.6356 | Acc: 81.93%\n",
            "Epoch 138 | Current lr: 0.0563 | Loss: 0.6227 | Acc: 82.45%\n",
            "Epoch 139 | Current lr: 0.0557 | Loss: 0.6278 | Acc: 81.53%\n",
            "Epoch 140 | Current lr: 0.0552 | Loss: 0.6263 | Acc: 81.59%\n",
            "Epoch 141 | Current lr: 0.0547 | Loss: 0.6196 | Acc: 80.85%\n",
            "Epoch 142 | Current lr: 0.0542 | Loss: 0.6191 | Acc: 81.61%\n",
            "Epoch 143 | Current lr: 0.0537 | Loss: 0.6191 | Acc: 81.37%\n",
            "Epoch 144 | Current lr: 0.0531 | Loss: 0.6203 | Acc: 82.66%\n",
            "Epoch 145 | Current lr: 0.0526 | Loss: 0.6106 | Acc: 81.93%\n",
            "Epoch 146 | Current lr: 0.0521 | Loss: 0.6061 | Acc: 83.22%\n",
            "Epoch 147 | Current lr: 0.0516 | Loss: 0.6081 | Acc: 80.87%\n",
            "Epoch 148 | Current lr: 0.0510 | Loss: 0.6012 | Acc: 79.94%\n",
            "Epoch 149 | Current lr: 0.0505 | Loss: 0.6029 | Acc: 80.56%\n",
            "Epoch 150 | Current lr: 0.0500 | Loss: 0.6007 | Acc: 82.98%\n",
            "Epoch 151 | Current lr: 0.0495 | Loss: 0.5970 | Acc: 82.43%\n",
            "Epoch 152 | Current lr: 0.0490 | Loss: 0.5882 | Acc: 81.76%\n",
            "Epoch 153 | Current lr: 0.0484 | Loss: 0.5891 | Acc: 82.53%\n",
            "Epoch 154 | Current lr: 0.0479 | Loss: 0.5879 | Acc: 82.77%\n",
            "Epoch 155 | Current lr: 0.0474 | Loss: 0.5845 | Acc: 82.87%\n",
            "Epoch 156 | Current lr: 0.0469 | Loss: 0.5807 | Acc: 82.70%\n",
            "Epoch 157 | Current lr: 0.0463 | Loss: 0.5781 | Acc: 81.14%\n",
            "Epoch 158 | Current lr: 0.0458 | Loss: 0.5806 | Acc: 79.44%\n",
            "Epoch 159 | Current lr: 0.0453 | Loss: 0.5806 | Acc: 81.66%\n",
            "Epoch 160 | Current lr: 0.0448 | Loss: 0.5700 | Acc: 81.85%\n",
            "Epoch 161 | Current lr: 0.0443 | Loss: 0.5768 | Acc: 83.26%\n",
            "Epoch 162 | Current lr: 0.0437 | Loss: 0.5687 | Acc: 84.38%\n",
            "Epoch 163 | Current lr: 0.0432 | Loss: 0.5633 | Acc: 82.90%\n",
            "Epoch 164 | Current lr: 0.0427 | Loss: 0.5610 | Acc: 83.27%\n",
            "Epoch 165 | Current lr: 0.0422 | Loss: 0.5613 | Acc: 83.79%\n",
            "Epoch 166 | Current lr: 0.0417 | Loss: 0.5513 | Acc: 83.05%\n",
            "Epoch 167 | Current lr: 0.0411 | Loss: 0.5604 | Acc: 83.21%\n",
            "Epoch 168 | Current lr: 0.0406 | Loss: 0.5488 | Acc: 82.26%\n",
            "Epoch 169 | Current lr: 0.0401 | Loss: 0.5484 | Acc: 83.79%\n",
            "Epoch 170 | Current lr: 0.0396 | Loss: 0.5366 | Acc: 84.19%\n",
            "Epoch 171 | Current lr: 0.0391 | Loss: 0.5428 | Acc: 83.33%\n",
            "Epoch 172 | Current lr: 0.0386 | Loss: 0.5447 | Acc: 82.42%\n",
            "Epoch 173 | Current lr: 0.0381 | Loss: 0.5376 | Acc: 83.06%\n",
            "Epoch 174 | Current lr: 0.0376 | Loss: 0.5338 | Acc: 83.96%\n",
            "Epoch 175 | Current lr: 0.0371 | Loss: 0.5348 | Acc: 84.00%\n",
            "Epoch 176 | Current lr: 0.0366 | Loss: 0.5231 | Acc: 82.80%\n",
            "Epoch 177 | Current lr: 0.0361 | Loss: 0.5286 | Acc: 84.63%\n",
            "Epoch 178 | Current lr: 0.0355 | Loss: 0.5221 | Acc: 84.82%\n",
            "Epoch 179 | Current lr: 0.0350 | Loss: 0.5167 | Acc: 83.69%\n",
            "Epoch 180 | Current lr: 0.0345 | Loss: 0.5104 | Acc: 84.26%\n",
            "Epoch 181 | Current lr: 0.0341 | Loss: 0.5103 | Acc: 83.96%\n",
            "Epoch 182 | Current lr: 0.0336 | Loss: 0.5071 | Acc: 84.31%\n",
            "Epoch 183 | Current lr: 0.0331 | Loss: 0.5096 | Acc: 83.20%\n",
            "Epoch 184 | Current lr: 0.0326 | Loss: 0.4997 | Acc: 82.98%\n",
            "Epoch 185 | Current lr: 0.0321 | Loss: 0.4972 | Acc: 83.03%\n",
            "Epoch 186 | Current lr: 0.0316 | Loss: 0.4988 | Acc: 84.89%\n",
            "Epoch 187 | Current lr: 0.0311 | Loss: 0.4892 | Acc: 81.77%\n",
            "Epoch 188 | Current lr: 0.0306 | Loss: 0.4961 | Acc: 84.49%\n",
            "Epoch 189 | Current lr: 0.0301 | Loss: 0.4882 | Acc: 84.19%\n",
            "Epoch 190 | Current lr: 0.0297 | Loss: 0.4840 | Acc: 84.69%\n",
            "Epoch 191 | Current lr: 0.0292 | Loss: 0.4787 | Acc: 84.40%\n",
            "Epoch 192 | Current lr: 0.0287 | Loss: 0.4747 | Acc: 83.97%\n",
            "Epoch 193 | Current lr: 0.0282 | Loss: 0.4778 | Acc: 84.87%\n",
            "Epoch 194 | Current lr: 0.0278 | Loss: 0.4690 | Acc: 85.05%\n",
            "Epoch 195 | Current lr: 0.0273 | Loss: 0.4651 | Acc: 85.85%\n",
            "Epoch 196 | Current lr: 0.0268 | Loss: 0.4649 | Acc: 84.88%\n",
            "Epoch 197 | Current lr: 0.0264 | Loss: 0.4619 | Acc: 85.47%\n",
            "Epoch 198 | Current lr: 0.0259 | Loss: 0.4514 | Acc: 84.41%\n",
            "Epoch 199 | Current lr: 0.0255 | Loss: 0.4517 | Acc: 84.03%\n",
            "Epoch 200 | Current lr: 0.0250 | Loss: 0.4527 | Acc: 85.16%\n",
            "Epoch 201 | Current lr: 0.0245 | Loss: 0.4419 | Acc: 84.61%\n",
            "Epoch 202 | Current lr: 0.0241 | Loss: 0.4432 | Acc: 85.78%\n",
            "Epoch 203 | Current lr: 0.0237 | Loss: 0.4374 | Acc: 86.64%\n",
            "Epoch 204 | Current lr: 0.0232 | Loss: 0.4336 | Acc: 86.29%\n",
            "Epoch 205 | Current lr: 0.0228 | Loss: 0.4320 | Acc: 85.88%\n",
            "Epoch 206 | Current lr: 0.0223 | Loss: 0.4296 | Acc: 87.12%\n",
            "Epoch 207 | Current lr: 0.0219 | Loss: 0.4297 | Acc: 86.01%\n",
            "Epoch 208 | Current lr: 0.0215 | Loss: 0.4285 | Acc: 85.19%\n",
            "Epoch 209 | Current lr: 0.0210 | Loss: 0.4157 | Acc: 86.30%\n",
            "Epoch 210 | Current lr: 0.0206 | Loss: 0.4212 | Acc: 86.55%\n",
            "Epoch 211 | Current lr: 0.0202 | Loss: 0.4110 | Acc: 85.99%\n",
            "Epoch 212 | Current lr: 0.0198 | Loss: 0.4062 | Acc: 85.99%\n",
            "Epoch 213 | Current lr: 0.0194 | Loss: 0.4032 | Acc: 86.45%\n",
            "Epoch 214 | Current lr: 0.0189 | Loss: 0.3965 | Acc: 86.60%\n",
            "Epoch 215 | Current lr: 0.0185 | Loss: 0.3965 | Acc: 87.07%\n",
            "Epoch 216 | Current lr: 0.0181 | Loss: 0.3903 | Acc: 85.70%\n",
            "Epoch 217 | Current lr: 0.0177 | Loss: 0.3882 | Acc: 87.73%\n",
            "Epoch 218 | Current lr: 0.0173 | Loss: 0.3845 | Acc: 86.90%\n",
            "Epoch 219 | Current lr: 0.0169 | Loss: 0.3803 | Acc: 86.94%\n",
            "Epoch 220 | Current lr: 0.0165 | Loss: 0.3754 | Acc: 86.27%\n",
            "Epoch 221 | Current lr: 0.0162 | Loss: 0.3683 | Acc: 86.79%\n",
            "Epoch 222 | Current lr: 0.0158 | Loss: 0.3667 | Acc: 86.92%\n",
            "Epoch 223 | Current lr: 0.0154 | Loss: 0.3687 | Acc: 86.78%\n",
            "Epoch 224 | Current lr: 0.0150 | Loss: 0.3622 | Acc: 86.84%\n",
            "Epoch 225 | Current lr: 0.0146 | Loss: 0.3498 | Acc: 85.76%\n",
            "Epoch 226 | Current lr: 0.0143 | Loss: 0.3548 | Acc: 86.89%\n",
            "Epoch 227 | Current lr: 0.0139 | Loss: 0.3441 | Acc: 86.91%\n",
            "Epoch 228 | Current lr: 0.0136 | Loss: 0.3408 | Acc: 87.51%\n",
            "Epoch 229 | Current lr: 0.0132 | Loss: 0.3391 | Acc: 87.01%\n",
            "Epoch 230 | Current lr: 0.0128 | Loss: 0.3430 | Acc: 87.60%\n",
            "Epoch 231 | Current lr: 0.0125 | Loss: 0.3295 | Acc: 87.47%\n",
            "Epoch 232 | Current lr: 0.0122 | Loss: 0.3260 | Acc: 87.27%\n",
            "Epoch 233 | Current lr: 0.0118 | Loss: 0.3215 | Acc: 87.74%\n",
            "Epoch 234 | Current lr: 0.0115 | Loss: 0.3182 | Acc: 87.01%\n",
            "Epoch 235 | Current lr: 0.0111 | Loss: 0.3157 | Acc: 88.15%\n",
            "Epoch 236 | Current lr: 0.0108 | Loss: 0.3100 | Acc: 87.79%\n",
            "Epoch 237 | Current lr: 0.0105 | Loss: 0.3101 | Acc: 86.88%\n",
            "Epoch 238 | Current lr: 0.0102 | Loss: 0.3032 | Acc: 87.71%\n",
            "Epoch 239 | Current lr: 0.0099 | Loss: 0.2999 | Acc: 87.35%\n",
            "Epoch 240 | Current lr: 0.0095 | Loss: 0.2895 | Acc: 87.58%\n",
            "Epoch 241 | Current lr: 0.0092 | Loss: 0.2867 | Acc: 88.19%\n",
            "Epoch 242 | Current lr: 0.0089 | Loss: 0.2838 | Acc: 88.27%\n",
            "Epoch 243 | Current lr: 0.0086 | Loss: 0.2794 | Acc: 88.38%\n",
            "Epoch 244 | Current lr: 0.0084 | Loss: 0.2715 | Acc: 88.20%\n",
            "Epoch 245 | Current lr: 0.0081 | Loss: 0.2685 | Acc: 88.09%\n",
            "Epoch 246 | Current lr: 0.0078 | Loss: 0.2652 | Acc: 88.52%\n",
            "Epoch 247 | Current lr: 0.0075 | Loss: 0.2648 | Acc: 88.66%\n",
            "Epoch 248 | Current lr: 0.0072 | Loss: 0.2557 | Acc: 88.18%\n",
            "Epoch 249 | Current lr: 0.0070 | Loss: 0.2565 | Acc: 88.31%\n",
            "Epoch 250 | Current lr: 0.0067 | Loss: 0.2442 | Acc: 88.07%\n",
            "Epoch 251 | Current lr: 0.0064 | Loss: 0.2482 | Acc: 88.75%\n",
            "Epoch 252 | Current lr: 0.0062 | Loss: 0.2401 | Acc: 89.19%\n",
            "Epoch 253 | Current lr: 0.0059 | Loss: 0.2330 | Acc: 88.90%\n",
            "Epoch 254 | Current lr: 0.0057 | Loss: 0.2277 | Acc: 88.65%\n",
            "Epoch 255 | Current lr: 0.0054 | Loss: 0.2232 | Acc: 88.93%\n",
            "Epoch 256 | Current lr: 0.0052 | Loss: 0.2159 | Acc: 88.96%\n",
            "Epoch 257 | Current lr: 0.0050 | Loss: 0.2165 | Acc: 88.83%\n",
            "Epoch 258 | Current lr: 0.0048 | Loss: 0.2119 | Acc: 88.17%\n",
            "Epoch 259 | Current lr: 0.0045 | Loss: 0.2052 | Acc: 88.78%\n",
            "Epoch 260 | Current lr: 0.0043 | Loss: 0.2050 | Acc: 88.92%\n",
            "Epoch 261 | Current lr: 0.0041 | Loss: 0.1953 | Acc: 88.96%\n",
            "Epoch 262 | Current lr: 0.0039 | Loss: 0.1978 | Acc: 89.19%\n",
            "Epoch 263 | Current lr: 0.0037 | Loss: 0.1870 | Acc: 88.70%\n",
            "Epoch 264 | Current lr: 0.0035 | Loss: 0.1830 | Acc: 89.50%\n",
            "Epoch 265 | Current lr: 0.0033 | Loss: 0.1857 | Acc: 89.33%\n",
            "Epoch 266 | Current lr: 0.0031 | Loss: 0.1791 | Acc: 89.25%\n",
            "Epoch 267 | Current lr: 0.0030 | Loss: 0.1766 | Acc: 89.19%\n",
            "Epoch 268 | Current lr: 0.0028 | Loss: 0.1690 | Acc: 89.43%\n",
            "Epoch 269 | Current lr: 0.0026 | Loss: 0.1676 | Acc: 89.26%\n",
            "Epoch 270 | Current lr: 0.0024 | Loss: 0.1672 | Acc: 89.70%\n",
            "Epoch 271 | Current lr: 0.0023 | Loss: 0.1583 | Acc: 89.52%\n",
            "Epoch 272 | Current lr: 0.0021 | Loss: 0.1623 | Acc: 89.70%\n",
            "Epoch 273 | Current lr: 0.0020 | Loss: 0.1548 | Acc: 89.72%\n",
            "Epoch 274 | Current lr: 0.0018 | Loss: 0.1527 | Acc: 89.69%\n",
            "Epoch 275 | Current lr: 0.0017 | Loss: 0.1484 | Acc: 89.95%\n",
            "Epoch 276 | Current lr: 0.0016 | Loss: 0.1459 | Acc: 89.78%\n",
            "Epoch 277 | Current lr: 0.0014 | Loss: 0.1440 | Acc: 89.78%\n",
            "Epoch 278 | Current lr: 0.0013 | Loss: 0.1418 | Acc: 90.00%\n",
            "Epoch 279 | Current lr: 0.0012 | Loss: 0.1378 | Acc: 90.01%\n",
            "Epoch 280 | Current lr: 0.0011 | Loss: 0.1352 | Acc: 89.91%\n",
            "Epoch 281 | Current lr: 0.0010 | Loss: 0.1353 | Acc: 89.64%\n",
            "Epoch 282 | Current lr: 0.0009 | Loss: 0.1363 | Acc: 89.95%\n",
            "Epoch 283 | Current lr: 0.0008 | Loss: 0.1336 | Acc: 89.89%\n",
            "Epoch 284 | Current lr: 0.0007 | Loss: 0.1268 | Acc: 90.17%\n",
            "Epoch 285 | Current lr: 0.0006 | Loss: 0.1279 | Acc: 90.03%\n",
            "Epoch 286 | Current lr: 0.0005 | Loss: 0.1323 | Acc: 89.91%\n",
            "Epoch 287 | Current lr: 0.0005 | Loss: 0.1212 | Acc: 89.95%\n",
            "Epoch 288 | Current lr: 0.0004 | Loss: 0.1234 | Acc: 90.04%\n",
            "Epoch 289 | Current lr: 0.0003 | Loss: 0.1225 | Acc: 90.14%\n",
            "Epoch 290 | Current lr: 0.0003 | Loss: 0.1220 | Acc: 90.09%\n",
            "Epoch 291 | Current lr: 0.0002 | Loss: 0.1215 | Acc: 89.97%\n",
            "Epoch 292 | Current lr: 0.0002 | Loss: 0.1177 | Acc: 90.01%\n",
            "Epoch 293 | Current lr: 0.0001 | Loss: 0.1199 | Acc: 90.11%\n",
            "Epoch 294 | Current lr: 0.0001 | Loss: 0.1173 | Acc: 89.98%\n",
            "Epoch 295 | Current lr: 0.0001 | Loss: 0.1181 | Acc: 89.98%\n",
            "Epoch 296 | Current lr: 0.0000 | Loss: 0.1176 | Acc: 90.12%\n",
            "Epoch 297 | Current lr: 0.0000 | Loss: 0.1190 | Acc: 90.12%\n",
            "Epoch 298 | Current lr: 0.0000 | Loss: 0.1217 | Acc: 90.02%\n",
            "Epoch 299 | Current lr: 0.0000 | Loss: 0.1186 | Acc: 90.06%\n",
            "Epoch 300 | Current lr: 0.0000 | Loss: 0.1205 | Acc: 90.03%\n",
            "\n",
            "Total training time: 6977.53 seconds\n"
          ]
        }
      ]
    }
  ]
}