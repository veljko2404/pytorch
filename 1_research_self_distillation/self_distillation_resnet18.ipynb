{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a ResNet18 using a Self-Distillation on CIFAR-10 dataset\n",
    " This Jupyter Notebook implements Self-Distillation, a specific form of Knowledge Distillation where the \"Student\" and the \"Teacher\" share the same architecture (ResNet18). Instead of a larger model teaching a smaller one, a previously trained version of the same model helps a new instance reach better performance or generalize more effectively on the CIFAR-10 dataset. Model was trained for **2 hours, 13 minutes, and 52 seconds** using **L4** graphics card on Google Colaboratory and achieved accuracy is **94.4%**\n",
    "\n",
    "#### Model Architecture\n",
    "- **Internal Classifiers**: The model attaches a bottleneck and a fully connected layer after each of the four main ResNet layers (layer1 through layer4).\n",
    "- **Bottlenecks**: Each internal layer is compressed into a 256-channel feature map before classification to keep the distillation consistent.\n",
    "- **Outputs**: The forward pass returns a list of 4 logits (predictions) and 4 feature maps (hidden states).\n",
    "#### Training Logic\n",
    "- **Teacher**: The final output (logits[-1]) and final features (feats[-1]) serve as the \"Teacher\".\n",
    "- **Student**: Layers 1, 2, and 3 act as \"Students\".\n",
    "\n",
    "#### Loss Components:\n",
    "- **Cross-Entropy (CE)**: Standard classification loss against the real labels.\n",
    "- **KL Divergence**: Forces the early layers' probability distributions to match the final layer's distribution (soft targets).\n",
    "- **MSE (L2) Loss**: Aligns the internal feature maps of earlier layers with the final layer's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_kowDkRzxkRs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCKlIBKsx_Aj",
    "outputId": "94b7325e-fc6e-4469-bea0-c9760fad7ca0"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 170M/170M [00:03<00:00, 44.1MB/s]\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### The `Bottleneck` Class\n",
    "This is a helper module used to standardize the data before it reaching the internal classifiers. It projects feature maps of different sizes (64, 128, 256, 512 channels) into a uniform space of 256 channels. It uses a kernel size of 1 to change the channel depth without altering the spatial height or width. Batch Normalization stabilizes the learning process.\n",
    "\n",
    "#### The `SelfDistillResNet18` Class\n",
    "This class builds the main backbone and the four distillation branches. The Stem (custom initial layer) is used instead of the standard ResNet stem to better handle the smaller 32x32 images of CIFAR-10. Layers 1-4 are the standard ResNet residual blocks inherited from the base model. Classifiers (fc1 to fc4) are four linear layers that map the 256-dimensional features to the 10 classes of CIFAR-10.\n",
    "\n",
    "#### Forward Pass (`forward`)\n",
    "The forward pass is designed to capture intermediate state information. The input $x$ passes through each layer sequentially (f1 $\\rightarrow$ f2 $\\rightarrow$ f3 $\\rightarrow$ f4). Global Average Pooling `F.adaptive_avg_pool2d(h, 1)` reduces the spatial dimensions to $1 \\times 1$ before the data is flattened for the linear layer. And it returns a list of 4 predictions (logits) and 4 feature maps, which are later used in the train_step to calculate the distillation loss."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(self.conv(x))\n",
    "\n",
    "class SelfDistillResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        base = resnet18(weights=None)\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = base.layer1 # 64\n",
    "        self.layer2 = base.layer2 # 128\n",
    "        self.layer3 = base.layer3 # 256\n",
    "        self.layer4 = base.layer4 # 512\n",
    "\n",
    "        # Bottlenecks\n",
    "        self.b1 = Bottleneck(64, 256)\n",
    "        self.b2 = Bottleneck(128, 256)\n",
    "        self.b3 = Bottleneck(256, 256)\n",
    "        self.b4 = Bottleneck(512, 256)\n",
    "\n",
    "        # Classifiers\n",
    "        self.fc1 = nn.Linear(256, num_classes)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.fc4 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "\n",
    "        f1 = self.layer1(x)\n",
    "        h1 = self.b1(f1)\n",
    "        p1 = self.fc1(F.adaptive_avg_pool2d(h1,1).flatten(1))\n",
    "\n",
    "        f2 = self.layer2(f1)\n",
    "        h2 = self.b2(f2)\n",
    "        p2 = self.fc2(F.adaptive_avg_pool2d(h2,1).flatten(1))\n",
    "\n",
    "        f3 = self.layer3(f2)\n",
    "        h3 = self.b3(f3)\n",
    "        p3 = self.fc3(F.adaptive_avg_pool2d(h3,1).flatten(1))\n",
    "\n",
    "        f4 = self.layer4(f3)\n",
    "        h4 = self.b4(f4)\n",
    "        p4 = self.fc4(F.adaptive_avg_pool2d(h4,1).flatten(1))\n",
    "\n",
    "        return [p1, p2, p3, p4], [h1, h2, h3, h4]"
   ],
   "metadata": {
    "id": "IDOujdjGyDzZ"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "CE = nn.CrossEntropyLoss()\n",
    "KL = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "MSE = nn.MSELoss()\n",
    "\n",
    "T = 4.0\n",
    "alpha = 0.7\n",
    "beta = 0.3\n",
    "\n",
    "def train_step(model, images, labels, optimizer):\n",
    "    logits, feats = model(images)\n",
    "\n",
    "    teacher_logits = logits[-1] # The deepest output (index -1) acts as the Teacher\n",
    "    teacher_feat = feats[-1].detach() # detach() is used so that we don't update the teacher based on the student\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(4):\n",
    "        student_logits = logits[i]\n",
    "        student_feat = feats[i]\n",
    "\n",
    "        loss_ce = CE(student_logits, labels)\n",
    "\n",
    "        if i == 3: # The final layer only learns from labels\n",
    "            total_loss += loss_ce\n",
    "            continue\n",
    "\n",
    "        # Forces early layers to mimic the teacher's probability distribution\n",
    "        log_p = F.log_softmax(student_logits / T, dim=1)\n",
    "        q = F.softmax(teacher_logits / T, dim=1)\n",
    "        loss_kl = KL(log_p, q) * (T*T)\n",
    "\n",
    "        # Forces early feature maps to align with the teacher's feature map\n",
    "        teacher_resized = F.interpolate(teacher_feat, size=student_feat.shape[2:], mode=\"bilinear\")\n",
    "        loss_l2 = MSE(student_feat, teacher_resized)\n",
    "\n",
    "        # Total loss is a weighted sum of CE, KL (alpha=0.7), and MSE (beta=0.3)\n",
    "        total_loss += loss_ce + alpha * loss_kl + beta * loss_l2\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return total_loss.item()"
   ],
   "metadata": {
    "id": "Rjf7TD8QyGyU"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lr = 0.1\n",
    "epochs = 200\n",
    "model = SelfDistillResNet18().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_epoch_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        loss = train_step(model, images, labels, optimizer)\n",
    "        total_epoch_loss += loss\n",
    "\n",
    "    avg_loss = total_epoch_loss / len(trainloader)\n",
    "\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits, _ = model(images)\n",
    "            final_outputs = logits[-1]\n",
    "            _, predicted = torch.max(final_outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1} | Current lr: {current_lr:.4f} | Loss: {avg_loss:.4f} | Accuracy: {acc:.2f}%\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal training time: {total_time:.2f} seconds\")\n",
    "\n",
    "torch.save(model.state_dict(), \"saved_models/self_distillation_resnet18.pt\")\n",
    "print(\"Model saved as saved_models/self_distillation_resnet18.pt\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfz7QDplyIUW",
    "outputId": "e35a9211-5ba2-4679-c013-7055a4fe69c9"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 | Current lr: 0.1000 | Loss: 8.0706 | Accuracy: 47.22%\n",
      "Epoch 2 | Current lr: 0.1000 | Loss: 6.4416 | Accuracy: 47.32%\n",
      "Epoch 3 | Current lr: 0.0999 | Loss: 5.6949 | Accuracy: 61.76%\n",
      "Epoch 4 | Current lr: 0.0999 | Loss: 5.2692 | Accuracy: 65.15%\n",
      "Epoch 5 | Current lr: 0.0998 | Loss: 4.9584 | Accuracy: 53.73%\n",
      "Epoch 6 | Current lr: 0.0998 | Loss: 4.7351 | Accuracy: 45.69%\n",
      "Epoch 7 | Current lr: 0.0997 | Loss: 4.6063 | Accuracy: 63.52%\n",
      "Epoch 8 | Current lr: 0.0996 | Loss: 4.4620 | Accuracy: 66.91%\n",
      "Epoch 9 | Current lr: 0.0995 | Loss: 4.3883 | Accuracy: 65.11%\n",
      "Epoch 10 | Current lr: 0.0994 | Loss: 4.2821 | Accuracy: 73.42%\n",
      "Epoch 11 | Current lr: 0.0993 | Loss: 4.2452 | Accuracy: 63.83%\n",
      "Epoch 12 | Current lr: 0.0991 | Loss: 4.1969 | Accuracy: 70.49%\n",
      "Epoch 13 | Current lr: 0.0990 | Loss: 4.1165 | Accuracy: 68.85%\n",
      "Epoch 14 | Current lr: 0.0988 | Loss: 4.1103 | Accuracy: 71.05%\n",
      "Epoch 15 | Current lr: 0.0986 | Loss: 4.0584 | Accuracy: 71.69%\n",
      "Epoch 16 | Current lr: 0.0984 | Loss: 4.0301 | Accuracy: 67.65%\n",
      "Epoch 17 | Current lr: 0.0982 | Loss: 4.0064 | Accuracy: 64.68%\n",
      "Epoch 18 | Current lr: 0.0980 | Loss: 4.0090 | Accuracy: 74.74%\n",
      "Epoch 19 | Current lr: 0.0978 | Loss: 3.9375 | Accuracy: 74.36%\n",
      "Epoch 20 | Current lr: 0.0976 | Loss: 3.9419 | Accuracy: 66.39%\n",
      "Epoch 21 | Current lr: 0.0973 | Loss: 3.9117 | Accuracy: 76.96%\n",
      "Epoch 22 | Current lr: 0.0970 | Loss: 3.8758 | Accuracy: 70.63%\n",
      "Epoch 23 | Current lr: 0.0968 | Loss: 3.8723 | Accuracy: 73.21%\n",
      "Epoch 24 | Current lr: 0.0965 | Loss: 3.8602 | Accuracy: 78.76%\n",
      "Epoch 25 | Current lr: 0.0962 | Loss: 3.8418 | Accuracy: 65.68%\n",
      "Epoch 26 | Current lr: 0.0959 | Loss: 3.7852 | Accuracy: 76.39%\n",
      "Epoch 27 | Current lr: 0.0956 | Loss: 3.8295 | Accuracy: 77.68%\n",
      "Epoch 28 | Current lr: 0.0952 | Loss: 3.7981 | Accuracy: 70.72%\n",
      "Epoch 29 | Current lr: 0.0949 | Loss: 3.8034 | Accuracy: 75.28%\n",
      "Epoch 30 | Current lr: 0.0946 | Loss: 3.7711 | Accuracy: 68.71%\n",
      "Epoch 31 | Current lr: 0.0942 | Loss: 3.7316 | Accuracy: 71.60%\n",
      "Epoch 32 | Current lr: 0.0938 | Loss: 3.7151 | Accuracy: 67.95%\n",
      "Epoch 33 | Current lr: 0.0934 | Loss: 3.7365 | Accuracy: 69.45%\n",
      "Epoch 34 | Current lr: 0.0930 | Loss: 3.7134 | Accuracy: 65.31%\n",
      "Epoch 35 | Current lr: 0.0926 | Loss: 3.7195 | Accuracy: 66.90%\n",
      "Epoch 36 | Current lr: 0.0922 | Loss: 3.6909 | Accuracy: 73.34%\n",
      "Epoch 37 | Current lr: 0.0918 | Loss: 3.6777 | Accuracy: 67.63%\n",
      "Epoch 38 | Current lr: 0.0914 | Loss: 3.6710 | Accuracy: 77.48%\n",
      "Epoch 39 | Current lr: 0.0909 | Loss: 3.6260 | Accuracy: 79.71%\n",
      "Epoch 40 | Current lr: 0.0905 | Loss: 3.6824 | Accuracy: 74.11%\n",
      "Epoch 41 | Current lr: 0.0900 | Loss: 3.6460 | Accuracy: 72.26%\n",
      "Epoch 42 | Current lr: 0.0895 | Loss: 3.6469 | Accuracy: 74.28%\n",
      "Epoch 43 | Current lr: 0.0890 | Loss: 3.6281 | Accuracy: 56.94%\n",
      "Epoch 44 | Current lr: 0.0885 | Loss: 3.6095 | Accuracy: 83.48%\n",
      "Epoch 45 | Current lr: 0.0880 | Loss: 3.6136 | Accuracy: 81.47%\n",
      "Epoch 46 | Current lr: 0.0875 | Loss: 3.5821 | Accuracy: 77.68%\n",
      "Epoch 47 | Current lr: 0.0870 | Loss: 3.5962 | Accuracy: 77.56%\n",
      "Epoch 48 | Current lr: 0.0864 | Loss: 3.5814 | Accuracy: 76.38%\n",
      "Epoch 49 | Current lr: 0.0859 | Loss: 3.5784 | Accuracy: 76.32%\n",
      "Epoch 50 | Current lr: 0.0854 | Loss: 3.5945 | Accuracy: 81.99%\n",
      "Epoch 51 | Current lr: 0.0848 | Loss: 3.5716 | Accuracy: 79.42%\n",
      "Epoch 52 | Current lr: 0.0842 | Loss: 3.5462 | Accuracy: 73.24%\n",
      "Epoch 53 | Current lr: 0.0837 | Loss: 3.5335 | Accuracy: 74.75%\n",
      "Epoch 54 | Current lr: 0.0831 | Loss: 3.5204 | Accuracy: 80.66%\n",
      "Epoch 55 | Current lr: 0.0825 | Loss: 3.5187 | Accuracy: 80.01%\n",
      "Epoch 56 | Current lr: 0.0819 | Loss: 3.5287 | Accuracy: 79.53%\n",
      "Epoch 57 | Current lr: 0.0813 | Loss: 3.4858 | Accuracy: 78.88%\n",
      "Epoch 58 | Current lr: 0.0806 | Loss: 3.4877 | Accuracy: 74.39%\n",
      "Epoch 59 | Current lr: 0.0800 | Loss: 3.4993 | Accuracy: 74.59%\n",
      "Epoch 60 | Current lr: 0.0794 | Loss: 3.4633 | Accuracy: 73.85%\n",
      "Epoch 61 | Current lr: 0.0788 | Loss: 3.4873 | Accuracy: 80.88%\n",
      "Epoch 62 | Current lr: 0.0781 | Loss: 3.4535 | Accuracy: 70.88%\n",
      "Epoch 63 | Current lr: 0.0775 | Loss: 3.4610 | Accuracy: 80.45%\n",
      "Epoch 64 | Current lr: 0.0768 | Loss: 3.4453 | Accuracy: 79.42%\n",
      "Epoch 65 | Current lr: 0.0761 | Loss: 3.4572 | Accuracy: 75.15%\n",
      "Epoch 66 | Current lr: 0.0755 | Loss: 3.4152 | Accuracy: 76.27%\n",
      "Epoch 67 | Current lr: 0.0748 | Loss: 3.4233 | Accuracy: 75.35%\n",
      "Epoch 68 | Current lr: 0.0741 | Loss: 3.4187 | Accuracy: 72.23%\n",
      "Epoch 69 | Current lr: 0.0734 | Loss: 3.4052 | Accuracy: 78.98%\n",
      "Epoch 70 | Current lr: 0.0727 | Loss: 3.4002 | Accuracy: 75.88%\n",
      "Epoch 71 | Current lr: 0.0720 | Loss: 3.3756 | Accuracy: 78.50%\n",
      "Epoch 72 | Current lr: 0.0713 | Loss: 3.3858 | Accuracy: 75.45%\n",
      "Epoch 73 | Current lr: 0.0706 | Loss: 3.3733 | Accuracy: 76.91%\n",
      "Epoch 74 | Current lr: 0.0699 | Loss: 3.3772 | Accuracy: 81.88%\n",
      "Epoch 75 | Current lr: 0.0691 | Loss: 3.3671 | Accuracy: 68.66%\n",
      "Epoch 76 | Current lr: 0.0684 | Loss: 3.3315 | Accuracy: 84.89%\n",
      "Epoch 77 | Current lr: 0.0677 | Loss: 3.3221 | Accuracy: 71.72%\n",
      "Epoch 78 | Current lr: 0.0669 | Loss: 3.3153 | Accuracy: 78.18%\n",
      "Epoch 79 | Current lr: 0.0662 | Loss: 3.3165 | Accuracy: 76.31%\n",
      "Epoch 80 | Current lr: 0.0655 | Loss: 3.3095 | Accuracy: 80.69%\n",
      "Epoch 81 | Current lr: 0.0647 | Loss: 3.2658 | Accuracy: 81.45%\n",
      "Epoch 82 | Current lr: 0.0639 | Loss: 3.3108 | Accuracy: 79.77%\n",
      "Epoch 83 | Current lr: 0.0632 | Loss: 3.2781 | Accuracy: 84.19%\n",
      "Epoch 84 | Current lr: 0.0624 | Loss: 3.2899 | Accuracy: 83.00%\n",
      "Epoch 85 | Current lr: 0.0617 | Loss: 3.2456 | Accuracy: 79.51%\n",
      "Epoch 86 | Current lr: 0.0609 | Loss: 3.2494 | Accuracy: 82.62%\n",
      "Epoch 87 | Current lr: 0.0601 | Loss: 3.2614 | Accuracy: 78.56%\n",
      "Epoch 88 | Current lr: 0.0594 | Loss: 3.2384 | Accuracy: 85.29%\n",
      "Epoch 89 | Current lr: 0.0586 | Loss: 3.2191 | Accuracy: 79.12%\n",
      "Epoch 90 | Current lr: 0.0578 | Loss: 3.2117 | Accuracy: 77.41%\n",
      "Epoch 91 | Current lr: 0.0570 | Loss: 3.2078 | Accuracy: 83.96%\n",
      "Epoch 92 | Current lr: 0.0563 | Loss: 3.2037 | Accuracy: 80.47%\n",
      "Epoch 93 | Current lr: 0.0555 | Loss: 3.2220 | Accuracy: 78.81%\n",
      "Epoch 94 | Current lr: 0.0547 | Loss: 3.1468 | Accuracy: 78.50%\n",
      "Epoch 95 | Current lr: 0.0539 | Loss: 3.1526 | Accuracy: 82.90%\n",
      "Epoch 96 | Current lr: 0.0531 | Loss: 3.1512 | Accuracy: 84.96%\n",
      "Epoch 97 | Current lr: 0.0524 | Loss: 3.1190 | Accuracy: 85.89%\n",
      "Epoch 98 | Current lr: 0.0516 | Loss: 3.1167 | Accuracy: 84.23%\n",
      "Epoch 99 | Current lr: 0.0508 | Loss: 3.0870 | Accuracy: 82.15%\n",
      "Epoch 100 | Current lr: 0.0500 | Loss: 3.1315 | Accuracy: 81.90%\n",
      "Epoch 101 | Current lr: 0.0492 | Loss: 3.0694 | Accuracy: 80.60%\n",
      "Epoch 102 | Current lr: 0.0484 | Loss: 3.0716 | Accuracy: 76.35%\n",
      "Epoch 103 | Current lr: 0.0476 | Loss: 3.0564 | Accuracy: 84.85%\n",
      "Epoch 104 | Current lr: 0.0469 | Loss: 3.0404 | Accuracy: 80.82%\n",
      "Epoch 105 | Current lr: 0.0461 | Loss: 3.0438 | Accuracy: 82.15%\n",
      "Epoch 106 | Current lr: 0.0453 | Loss: 3.0208 | Accuracy: 79.88%\n",
      "Epoch 107 | Current lr: 0.0445 | Loss: 3.0199 | Accuracy: 81.15%\n",
      "Epoch 108 | Current lr: 0.0437 | Loss: 3.0213 | Accuracy: 75.50%\n",
      "Epoch 109 | Current lr: 0.0430 | Loss: 3.0021 | Accuracy: 82.91%\n",
      "Epoch 110 | Current lr: 0.0422 | Loss: 2.9782 | Accuracy: 85.94%\n",
      "Epoch 111 | Current lr: 0.0414 | Loss: 2.9884 | Accuracy: 84.62%\n",
      "Epoch 112 | Current lr: 0.0406 | Loss: 2.9694 | Accuracy: 78.99%\n",
      "Epoch 113 | Current lr: 0.0399 | Loss: 2.9491 | Accuracy: 81.27%\n",
      "Epoch 114 | Current lr: 0.0391 | Loss: 2.9385 | Accuracy: 84.66%\n",
      "Epoch 115 | Current lr: 0.0383 | Loss: 2.9501 | Accuracy: 84.64%\n",
      "Epoch 116 | Current lr: 0.0376 | Loss: 2.9109 | Accuracy: 83.61%\n",
      "Epoch 117 | Current lr: 0.0368 | Loss: 2.8987 | Accuracy: 86.67%\n",
      "Epoch 118 | Current lr: 0.0361 | Loss: 2.8839 | Accuracy: 82.50%\n",
      "Epoch 119 | Current lr: 0.0353 | Loss: 2.8859 | Accuracy: 86.62%\n",
      "Epoch 120 | Current lr: 0.0345 | Loss: 2.8618 | Accuracy: 89.12%\n",
      "Epoch 121 | Current lr: 0.0338 | Loss: 2.8578 | Accuracy: 84.08%\n",
      "Epoch 122 | Current lr: 0.0331 | Loss: 2.8514 | Accuracy: 76.27%\n",
      "Epoch 123 | Current lr: 0.0323 | Loss: 2.8198 | Accuracy: 86.58%\n",
      "Epoch 124 | Current lr: 0.0316 | Loss: 2.8321 | Accuracy: 85.62%\n",
      "Epoch 125 | Current lr: 0.0309 | Loss: 2.7800 | Accuracy: 83.08%\n",
      "Epoch 126 | Current lr: 0.0301 | Loss: 2.7968 | Accuracy: 84.53%\n",
      "Epoch 127 | Current lr: 0.0294 | Loss: 2.7784 | Accuracy: 82.72%\n",
      "Epoch 128 | Current lr: 0.0287 | Loss: 2.7653 | Accuracy: 84.20%\n",
      "Epoch 129 | Current lr: 0.0280 | Loss: 2.7457 | Accuracy: 86.89%\n",
      "Epoch 130 | Current lr: 0.0273 | Loss: 2.7357 | Accuracy: 85.58%\n",
      "Epoch 131 | Current lr: 0.0266 | Loss: 2.7263 | Accuracy: 87.26%\n",
      "Epoch 132 | Current lr: 0.0259 | Loss: 2.6905 | Accuracy: 87.32%\n",
      "Epoch 133 | Current lr: 0.0252 | Loss: 2.6971 | Accuracy: 87.69%\n",
      "Epoch 134 | Current lr: 0.0245 | Loss: 2.6688 | Accuracy: 82.90%\n",
      "Epoch 135 | Current lr: 0.0239 | Loss: 2.6579 | Accuracy: 87.63%\n",
      "Epoch 136 | Current lr: 0.0232 | Loss: 2.6478 | Accuracy: 85.56%\n",
      "Epoch 137 | Current lr: 0.0225 | Loss: 2.6490 | Accuracy: 89.31%\n",
      "Epoch 138 | Current lr: 0.0219 | Loss: 2.6517 | Accuracy: 88.35%\n",
      "Epoch 139 | Current lr: 0.0212 | Loss: 2.6086 | Accuracy: 88.22%\n",
      "Epoch 140 | Current lr: 0.0206 | Loss: 2.5896 | Accuracy: 85.38%\n",
      "Epoch 141 | Current lr: 0.0200 | Loss: 2.5729 | Accuracy: 89.56%\n",
      "Epoch 142 | Current lr: 0.0194 | Loss: 2.5625 | Accuracy: 84.66%\n",
      "Epoch 143 | Current lr: 0.0187 | Loss: 2.5317 | Accuracy: 87.72%\n",
      "Epoch 144 | Current lr: 0.0181 | Loss: 2.5259 | Accuracy: 89.13%\n",
      "Epoch 145 | Current lr: 0.0175 | Loss: 2.5198 | Accuracy: 82.87%\n",
      "Epoch 146 | Current lr: 0.0169 | Loss: 2.4882 | Accuracy: 88.23%\n",
      "Epoch 147 | Current lr: 0.0163 | Loss: 2.4841 | Accuracy: 88.94%\n",
      "Epoch 148 | Current lr: 0.0158 | Loss: 2.4802 | Accuracy: 88.69%\n",
      "Epoch 149 | Current lr: 0.0152 | Loss: 2.4447 | Accuracy: 88.19%\n",
      "Epoch 150 | Current lr: 0.0146 | Loss: 2.4166 | Accuracy: 90.08%\n",
      "Epoch 151 | Current lr: 0.0141 | Loss: 2.4394 | Accuracy: 88.03%\n",
      "Epoch 152 | Current lr: 0.0136 | Loss: 2.3706 | Accuracy: 87.55%\n",
      "Epoch 153 | Current lr: 0.0130 | Loss: 2.3884 | Accuracy: 90.79%\n",
      "Epoch 154 | Current lr: 0.0125 | Loss: 2.3597 | Accuracy: 88.49%\n",
      "Epoch 155 | Current lr: 0.0120 | Loss: 2.3503 | Accuracy: 91.25%\n",
      "Epoch 156 | Current lr: 0.0115 | Loss: 2.3380 | Accuracy: 91.51%\n",
      "Epoch 157 | Current lr: 0.0110 | Loss: 2.3201 | Accuracy: 89.13%\n",
      "Epoch 158 | Current lr: 0.0105 | Loss: 2.3077 | Accuracy: 89.41%\n",
      "Epoch 159 | Current lr: 0.0100 | Loss: 2.2585 | Accuracy: 88.33%\n",
      "Epoch 160 | Current lr: 0.0095 | Loss: 2.2480 | Accuracy: 90.56%\n",
      "Epoch 161 | Current lr: 0.0091 | Loss: 2.2529 | Accuracy: 91.09%\n",
      "Epoch 162 | Current lr: 0.0086 | Loss: 2.2287 | Accuracy: 89.94%\n",
      "Epoch 163 | Current lr: 0.0082 | Loss: 2.2229 | Accuracy: 89.93%\n",
      "Epoch 164 | Current lr: 0.0078 | Loss: 2.1882 | Accuracy: 91.75%\n",
      "Epoch 165 | Current lr: 0.0074 | Loss: 2.1663 | Accuracy: 89.35%\n",
      "Epoch 166 | Current lr: 0.0070 | Loss: 2.1403 | Accuracy: 90.04%\n",
      "Epoch 167 | Current lr: 0.0066 | Loss: 2.0988 | Accuracy: 92.12%\n",
      "Epoch 168 | Current lr: 0.0062 | Loss: 2.1088 | Accuracy: 91.85%\n",
      "Epoch 169 | Current lr: 0.0058 | Loss: 2.0876 | Accuracy: 92.19%\n",
      "Epoch 170 | Current lr: 0.0054 | Loss: 2.0689 | Accuracy: 90.85%\n",
      "Epoch 171 | Current lr: 0.0051 | Loss: 2.0684 | Accuracy: 92.56%\n",
      "Epoch 172 | Current lr: 0.0048 | Loss: 2.0266 | Accuracy: 92.51%\n",
      "Epoch 173 | Current lr: 0.0044 | Loss: 2.0314 | Accuracy: 93.29%\n",
      "Epoch 174 | Current lr: 0.0041 | Loss: 2.0131 | Accuracy: 92.50%\n",
      "Epoch 175 | Current lr: 0.0038 | Loss: 1.9976 | Accuracy: 92.80%\n",
      "Epoch 176 | Current lr: 0.0035 | Loss: 1.9603 | Accuracy: 93.08%\n",
      "Epoch 177 | Current lr: 0.0032 | Loss: 1.9492 | Accuracy: 93.08%\n",
      "Epoch 178 | Current lr: 0.0030 | Loss: 1.9210 | Accuracy: 93.45%\n",
      "Epoch 179 | Current lr: 0.0027 | Loss: 1.9008 | Accuracy: 93.33%\n",
      "Epoch 180 | Current lr: 0.0024 | Loss: 1.8968 | Accuracy: 93.70%\n",
      "Epoch 181 | Current lr: 0.0022 | Loss: 1.8789 | Accuracy: 93.32%\n",
      "Epoch 182 | Current lr: 0.0020 | Loss: 1.8568 | Accuracy: 93.71%\n",
      "Epoch 183 | Current lr: 0.0018 | Loss: 1.8525 | Accuracy: 93.39%\n",
      "Epoch 184 | Current lr: 0.0016 | Loss: 1.8383 | Accuracy: 93.63%\n",
      "Epoch 185 | Current lr: 0.0014 | Loss: 1.8283 | Accuracy: 93.86%\n",
      "Epoch 186 | Current lr: 0.0012 | Loss: 1.8018 | Accuracy: 94.08%\n",
      "Epoch 187 | Current lr: 0.0010 | Loss: 1.7996 | Accuracy: 93.98%\n",
      "Epoch 188 | Current lr: 0.0009 | Loss: 1.7786 | Accuracy: 93.73%\n",
      "Epoch 189 | Current lr: 0.0007 | Loss: 1.7649 | Accuracy: 93.60%\n",
      "Epoch 190 | Current lr: 0.0006 | Loss: 1.7570 | Accuracy: 94.13%\n",
      "Epoch 191 | Current lr: 0.0005 | Loss: 1.7593 | Accuracy: 93.88%\n",
      "Epoch 192 | Current lr: 0.0004 | Loss: 1.7463 | Accuracy: 94.27%\n",
      "Epoch 193 | Current lr: 0.0003 | Loss: 1.7358 | Accuracy: 94.21%\n",
      "Epoch 194 | Current lr: 0.0002 | Loss: 1.7302 | Accuracy: 94.24%\n",
      "Epoch 195 | Current lr: 0.0002 | Loss: 1.7098 | Accuracy: 94.13%\n",
      "Epoch 196 | Current lr: 0.0001 | Loss: 1.7229 | Accuracy: 94.35%\n",
      "Epoch 197 | Current lr: 0.0001 | Loss: 1.7219 | Accuracy: 94.38%\n",
      "Epoch 198 | Current lr: 0.0000 | Loss: 1.7077 | Accuracy: 94.34%\n",
      "Epoch 199 | Current lr: 0.0000 | Loss: 1.7175 | Accuracy: 94.26%\n",
      "Epoch 200 | Current lr: 0.0000 | Loss: 1.7255 | Accuracy: 94.41%\n",
      "\n",
      "Total training time: 8032.52 seconds\n",
      "Model saved as saved_models/self_distillation_resnet18.pt\n"
     ]
    }
   ]
  }
 ]
}
