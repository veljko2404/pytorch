{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a ResNet18 Student using Knowledge Distillation from Teacher ResNet50 on CIFAR-10 dataset\n",
    "\n",
    "Knowledge Distillation is a technique where a smaller \"Student\" model (ResNet18) learns to mimic the behavior of a pre-trained, larger \"Teacher\" model (ResNet50). Instead of the Student learning only from hard labels e.g. \"this is a cat\", it learns from the Teacher’s full probability distribution e.g. \"this is 90% a cat, but it has 8% features of a dog and 2% features of a truck\".\n",
    "\n",
    "Model was trained for **2 hours, 13 minutes, and 6 seconds** using **L4** graphics card on Google Colaboratory and achieved accuracy is **94.2%**\n",
    "\n",
    "#### Core Components of the Process\n",
    "- **Soft Targets & Temperature ($T$)**: In standard training, the final layer uses a Softmax function to produce probabilities. By introducing Temperature ($T > 1$), we \"soften\" the output. This reveals the relative similarities between classes that are otherwise hidden in a standard 0 or 1 classification.\n",
    "- **The Teacher Model (ResNet50)**: This model is pre-trained and its weights are frozen `(teacher.eval())`. It acts as a guide, providing high-quality \"soft targets\" for the Student to mimic.\n",
    "- **The Student Model (ResNet18)**: This is the model being trained. It is significantly smaller and faster, making it ideal for deployment on mobile devices or edge hardware.\n",
    "\n",
    "#### The Two-Part Loss Function\n",
    "The Student model doesn't just listen to the Teacher; it also looks at the actual labels. The total loss is a weighted sum:\n",
    "- **Distillation Loss**: KL Divergence between the Student's and Teacher's soft outputs. Forces the Student to learn the internal logic and \"mistakes\" of the Teacher.\n",
    "- **Student Loss**: Standard Cross-Entropy between Student predictions and ground-truth labels. Ensures the Student still learns the correct final answers from the dataset.\n",
    "\n",
    "**Alpha** ($\\alpha = 0.5$): This balances the two losses. A value of 0.5 means the Student cares equally about the Teacher's advice and the actual ground truth.\n",
    "\n",
    "**Temperature** ($T = 3.0$): This \"stretches\" the probability distribution. A higher $T$ makes the distribution flatter, helping the Student see which \"wrong\" classes the Teacher thinks are somewhat similar to the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h76lyNDcI6Ng"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "lr = 0.1\n",
    "temperature = 3.0 # Softens the teacher's probability distribution\n",
    "alpha = 0.5 # Balances the loss between teacher guidance and true labels\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8hqDikrJPr_",
    "outputId": "0abf5167-52da-4ab8-f0eb-c6a7b7cb65ac"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 170M/170M [00:03<00:00, 44.5MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "teacher = torchvision.models.resnet50(num_classes=10)\n",
    "\n",
    "# Load pre-trained weights\n",
    "teacher.load_state_dict(torch.load('saved_models/teacher_resnet50.pt', map_location=device))\n",
    "\n",
    "# Modify architecture for CIFAR-10: 3x3 kernel and no maxpool to preserve spatial info\n",
    "teacher.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "teacher.maxpool = nn.Identity()\n",
    "\n",
    "teacher.to(device)\n",
    "teacher.eval() # Teacher is in evaluation mode all the time\n",
    "\n",
    "student = torchvision.models.resnet18(num_classes=10)\n",
    "student.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "student.maxpool = nn.Identity()\n",
    "student.to(device)\n",
    "\n",
    "optimizer = optim.SGD(student.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T, alpha):\n",
    "    student_ce_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "    soft_teacher = F.softmax(teacher_logits / T, dim=1)\n",
    "    soft_student = F.log_softmax(student_logits / T, dim=1)\n",
    "    distill_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T * T)\n",
    "\n",
    "    return alpha * distill_loss + (1 - alpha) * student_ce_loss"
   ],
   "metadata": {
    "id": "WnUEtAfRJXYg"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    student.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        student_outputs = student(inputs) # Forward pass through the student\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs) # Forward pass through the teacher\n",
    "\n",
    "        # Calculate the distillation loss\n",
    "        loss = distillation_loss(student_outputs, teacher_outputs, targets, temperature, alpha)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() # training statistics\n",
    "        _, predicted = student_outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    scheduler.step() # Update the learning rate\n",
    "\n",
    "    student.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = student(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100. * test_correct / test_total\n",
    "    print(f'Epoch {epoch+1} | Current lr: {scheduler.get_last_lr()[0]:.4f} | Loss: {train_loss/(batch_idx+1):.4f} | Test Acc: {acc:.2f}%')\n",
    "\n",
    "torch.save(student.state_dict(), \"saved_models/student_resnet18.pt\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal training time: {total_time:.2f} seconds\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ks_lL68KWlt",
    "outputId": "5d1dcd16-19f3-4f40-eab3-31aff43b3567"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 | Current lr: 0.1000 | Loss: 1.4895 | Test Acc: 23.32%\n",
      "Epoch 2 | Current lr: 0.1000 | Loss: 1.2001 | Test Acc: 39.96%\n",
      "Epoch 3 | Current lr: 0.0999 | Loss: 1.0930 | Test Acc: 50.43%\n",
      "Epoch 4 | Current lr: 0.0999 | Loss: 1.0191 | Test Acc: 58.48%\n",
      "Epoch 5 | Current lr: 0.0998 | Loss: 0.9681 | Test Acc: 63.07%\n",
      "Epoch 6 | Current lr: 0.0998 | Loss: 0.9293 | Test Acc: 65.32%\n",
      "Epoch 7 | Current lr: 0.0997 | Loss: 0.8929 | Test Acc: 67.18%\n",
      "Epoch 8 | Current lr: 0.0996 | Loss: 0.8620 | Test Acc: 71.76%\n",
      "Epoch 9 | Current lr: 0.0995 | Loss: 0.8441 | Test Acc: 71.43%\n",
      "Epoch 10 | Current lr: 0.0994 | Loss: 0.8330 | Test Acc: 71.55%\n",
      "Epoch 11 | Current lr: 0.0993 | Loss: 0.8227 | Test Acc: 69.86%\n",
      "Epoch 12 | Current lr: 0.0991 | Loss: 0.8162 | Test Acc: 76.87%\n",
      "Epoch 13 | Current lr: 0.0990 | Loss: 0.8082 | Test Acc: 66.65%\n",
      "Epoch 14 | Current lr: 0.0988 | Loss: 0.8029 | Test Acc: 72.38%\n",
      "Epoch 15 | Current lr: 0.0986 | Loss: 0.7950 | Test Acc: 73.06%\n",
      "Epoch 16 | Current lr: 0.0984 | Loss: 0.7904 | Test Acc: 69.66%\n",
      "Epoch 17 | Current lr: 0.0982 | Loss: 0.7833 | Test Acc: 72.77%\n",
      "Epoch 18 | Current lr: 0.0980 | Loss: 0.7828 | Test Acc: 82.18%\n",
      "Epoch 19 | Current lr: 0.0978 | Loss: 0.7791 | Test Acc: 80.22%\n",
      "Epoch 20 | Current lr: 0.0976 | Loss: 0.7748 | Test Acc: 79.97%\n",
      "Epoch 21 | Current lr: 0.0973 | Loss: 0.7699 | Test Acc: 78.23%\n",
      "Epoch 22 | Current lr: 0.0970 | Loss: 0.7702 | Test Acc: 75.90%\n",
      "Epoch 23 | Current lr: 0.0968 | Loss: 0.7650 | Test Acc: 75.32%\n",
      "Epoch 24 | Current lr: 0.0965 | Loss: 0.7632 | Test Acc: 69.55%\n",
      "Epoch 25 | Current lr: 0.0962 | Loss: 0.7631 | Test Acc: 78.95%\n",
      "Epoch 26 | Current lr: 0.0959 | Loss: 0.7605 | Test Acc: 75.70%\n",
      "Epoch 27 | Current lr: 0.0956 | Loss: 0.7581 | Test Acc: 81.74%\n",
      "Epoch 28 | Current lr: 0.0952 | Loss: 0.7589 | Test Acc: 80.10%\n",
      "Epoch 29 | Current lr: 0.0949 | Loss: 0.7545 | Test Acc: 80.49%\n",
      "Epoch 30 | Current lr: 0.0946 | Loss: 0.7555 | Test Acc: 80.74%\n",
      "Epoch 31 | Current lr: 0.0942 | Loss: 0.7500 | Test Acc: 80.02%\n",
      "Epoch 32 | Current lr: 0.0938 | Loss: 0.7507 | Test Acc: 81.80%\n",
      "Epoch 33 | Current lr: 0.0934 | Loss: 0.7512 | Test Acc: 82.26%\n",
      "Epoch 34 | Current lr: 0.0930 | Loss: 0.7498 | Test Acc: 81.57%\n",
      "Epoch 35 | Current lr: 0.0926 | Loss: 0.7488 | Test Acc: 81.15%\n",
      "Epoch 36 | Current lr: 0.0922 | Loss: 0.7448 | Test Acc: 78.01%\n",
      "Epoch 37 | Current lr: 0.0918 | Loss: 0.7464 | Test Acc: 81.59%\n",
      "Epoch 38 | Current lr: 0.0914 | Loss: 0.7435 | Test Acc: 82.80%\n",
      "Epoch 39 | Current lr: 0.0909 | Loss: 0.7458 | Test Acc: 79.01%\n",
      "Epoch 40 | Current lr: 0.0905 | Loss: 0.7423 | Test Acc: 82.13%\n",
      "Epoch 41 | Current lr: 0.0900 | Loss: 0.7425 | Test Acc: 80.32%\n",
      "Epoch 42 | Current lr: 0.0895 | Loss: 0.7443 | Test Acc: 80.47%\n",
      "Epoch 43 | Current lr: 0.0890 | Loss: 0.7413 | Test Acc: 82.16%\n",
      "Epoch 44 | Current lr: 0.0885 | Loss: 0.7377 | Test Acc: 81.68%\n",
      "Epoch 45 | Current lr: 0.0880 | Loss: 0.7400 | Test Acc: 82.07%\n",
      "Epoch 46 | Current lr: 0.0875 | Loss: 0.7390 | Test Acc: 81.60%\n",
      "Epoch 47 | Current lr: 0.0870 | Loss: 0.7382 | Test Acc: 81.96%\n",
      "Epoch 48 | Current lr: 0.0864 | Loss: 0.7373 | Test Acc: 79.47%\n",
      "Epoch 49 | Current lr: 0.0859 | Loss: 0.7353 | Test Acc: 83.68%\n",
      "Epoch 50 | Current lr: 0.0854 | Loss: 0.7364 | Test Acc: 75.67%\n",
      "Epoch 51 | Current lr: 0.0848 | Loss: 0.7352 | Test Acc: 81.61%\n",
      "Epoch 52 | Current lr: 0.0842 | Loss: 0.7340 | Test Acc: 74.38%\n",
      "Epoch 53 | Current lr: 0.0837 | Loss: 0.7354 | Test Acc: 83.80%\n",
      "Epoch 54 | Current lr: 0.0831 | Loss: 0.7311 | Test Acc: 82.80%\n",
      "Epoch 55 | Current lr: 0.0825 | Loss: 0.7315 | Test Acc: 85.55%\n",
      "Epoch 56 | Current lr: 0.0819 | Loss: 0.7293 | Test Acc: 84.83%\n",
      "Epoch 57 | Current lr: 0.0813 | Loss: 0.7308 | Test Acc: 84.16%\n",
      "Epoch 58 | Current lr: 0.0806 | Loss: 0.7278 | Test Acc: 79.09%\n",
      "Epoch 59 | Current lr: 0.0800 | Loss: 0.7300 | Test Acc: 83.38%\n",
      "Epoch 60 | Current lr: 0.0794 | Loss: 0.7276 | Test Acc: 78.89%\n",
      "Epoch 61 | Current lr: 0.0788 | Loss: 0.7261 | Test Acc: 83.32%\n",
      "Epoch 62 | Current lr: 0.0781 | Loss: 0.7259 | Test Acc: 81.13%\n",
      "Epoch 63 | Current lr: 0.0775 | Loss: 0.7241 | Test Acc: 82.83%\n",
      "Epoch 64 | Current lr: 0.0768 | Loss: 0.7274 | Test Acc: 83.83%\n",
      "Epoch 65 | Current lr: 0.0761 | Loss: 0.7237 | Test Acc: 79.27%\n",
      "Epoch 66 | Current lr: 0.0755 | Loss: 0.7224 | Test Acc: 82.16%\n",
      "Epoch 67 | Current lr: 0.0748 | Loss: 0.7214 | Test Acc: 77.51%\n",
      "Epoch 68 | Current lr: 0.0741 | Loss: 0.7217 | Test Acc: 83.26%\n",
      "Epoch 69 | Current lr: 0.0734 | Loss: 0.7199 | Test Acc: 83.67%\n",
      "Epoch 70 | Current lr: 0.0727 | Loss: 0.7199 | Test Acc: 82.71%\n",
      "Epoch 71 | Current lr: 0.0720 | Loss: 0.7183 | Test Acc: 85.31%\n",
      "Epoch 72 | Current lr: 0.0713 | Loss: 0.7173 | Test Acc: 84.94%\n",
      "Epoch 73 | Current lr: 0.0706 | Loss: 0.7162 | Test Acc: 86.54%\n",
      "Epoch 74 | Current lr: 0.0699 | Loss: 0.7165 | Test Acc: 85.56%\n",
      "Epoch 75 | Current lr: 0.0691 | Loss: 0.7153 | Test Acc: 82.48%\n",
      "Epoch 76 | Current lr: 0.0684 | Loss: 0.7141 | Test Acc: 85.37%\n",
      "Epoch 77 | Current lr: 0.0677 | Loss: 0.7154 | Test Acc: 80.31%\n",
      "Epoch 78 | Current lr: 0.0669 | Loss: 0.7110 | Test Acc: 81.85%\n",
      "Epoch 79 | Current lr: 0.0662 | Loss: 0.7143 | Test Acc: 85.62%\n",
      "Epoch 80 | Current lr: 0.0655 | Loss: 0.7125 | Test Acc: 85.47%\n",
      "Epoch 81 | Current lr: 0.0647 | Loss: 0.7094 | Test Acc: 84.61%\n",
      "Epoch 82 | Current lr: 0.0639 | Loss: 0.7075 | Test Acc: 86.95%\n",
      "Epoch 83 | Current lr: 0.0632 | Loss: 0.7097 | Test Acc: 85.95%\n",
      "Epoch 84 | Current lr: 0.0624 | Loss: 0.7048 | Test Acc: 84.87%\n",
      "Epoch 85 | Current lr: 0.0617 | Loss: 0.7066 | Test Acc: 83.72%\n",
      "Epoch 86 | Current lr: 0.0609 | Loss: 0.7034 | Test Acc: 85.84%\n",
      "Epoch 87 | Current lr: 0.0601 | Loss: 0.7048 | Test Acc: 84.18%\n",
      "Epoch 88 | Current lr: 0.0594 | Loss: 0.7029 | Test Acc: 83.03%\n",
      "Epoch 89 | Current lr: 0.0586 | Loss: 0.7030 | Test Acc: 84.41%\n",
      "Epoch 90 | Current lr: 0.0578 | Loss: 0.7000 | Test Acc: 88.61%\n",
      "Epoch 91 | Current lr: 0.0570 | Loss: 0.6996 | Test Acc: 86.01%\n",
      "Epoch 92 | Current lr: 0.0563 | Loss: 0.6991 | Test Acc: 86.14%\n",
      "Epoch 93 | Current lr: 0.0555 | Loss: 0.6966 | Test Acc: 85.86%\n",
      "Epoch 94 | Current lr: 0.0547 | Loss: 0.6987 | Test Acc: 85.33%\n",
      "Epoch 95 | Current lr: 0.0539 | Loss: 0.6972 | Test Acc: 84.93%\n",
      "Epoch 96 | Current lr: 0.0531 | Loss: 0.6953 | Test Acc: 86.34%\n",
      "Epoch 97 | Current lr: 0.0524 | Loss: 0.6928 | Test Acc: 88.65%\n",
      "Epoch 98 | Current lr: 0.0516 | Loss: 0.6932 | Test Acc: 83.84%\n",
      "Epoch 99 | Current lr: 0.0508 | Loss: 0.6903 | Test Acc: 85.31%\n",
      "Epoch 100 | Current lr: 0.0500 | Loss: 0.6908 | Test Acc: 84.37%\n",
      "Epoch 101 | Current lr: 0.0492 | Loss: 0.6919 | Test Acc: 85.99%\n",
      "Epoch 102 | Current lr: 0.0484 | Loss: 0.6907 | Test Acc: 87.45%\n",
      "Epoch 103 | Current lr: 0.0476 | Loss: 0.6875 | Test Acc: 88.62%\n",
      "Epoch 104 | Current lr: 0.0469 | Loss: 0.6887 | Test Acc: 87.12%\n",
      "Epoch 105 | Current lr: 0.0461 | Loss: 0.6848 | Test Acc: 87.73%\n",
      "Epoch 106 | Current lr: 0.0453 | Loss: 0.6867 | Test Acc: 87.76%\n",
      "Epoch 107 | Current lr: 0.0445 | Loss: 0.6840 | Test Acc: 87.88%\n",
      "Epoch 108 | Current lr: 0.0437 | Loss: 0.6819 | Test Acc: 86.96%\n",
      "Epoch 109 | Current lr: 0.0430 | Loss: 0.6814 | Test Acc: 85.14%\n",
      "Epoch 110 | Current lr: 0.0422 | Loss: 0.6801 | Test Acc: 85.36%\n",
      "Epoch 111 | Current lr: 0.0414 | Loss: 0.6793 | Test Acc: 89.81%\n",
      "Epoch 112 | Current lr: 0.0406 | Loss: 0.6782 | Test Acc: 88.15%\n",
      "Epoch 113 | Current lr: 0.0399 | Loss: 0.6778 | Test Acc: 85.92%\n",
      "Epoch 114 | Current lr: 0.0391 | Loss: 0.6763 | Test Acc: 86.80%\n",
      "Epoch 115 | Current lr: 0.0383 | Loss: 0.6736 | Test Acc: 89.05%\n",
      "Epoch 116 | Current lr: 0.0376 | Loss: 0.6725 | Test Acc: 87.17%\n",
      "Epoch 117 | Current lr: 0.0368 | Loss: 0.6722 | Test Acc: 86.78%\n",
      "Epoch 118 | Current lr: 0.0361 | Loss: 0.6705 | Test Acc: 86.94%\n",
      "Epoch 119 | Current lr: 0.0353 | Loss: 0.6685 | Test Acc: 87.19%\n",
      "Epoch 120 | Current lr: 0.0345 | Loss: 0.6691 | Test Acc: 88.67%\n",
      "Epoch 121 | Current lr: 0.0338 | Loss: 0.6683 | Test Acc: 88.55%\n",
      "Epoch 122 | Current lr: 0.0331 | Loss: 0.6661 | Test Acc: 85.32%\n",
      "Epoch 123 | Current lr: 0.0323 | Loss: 0.6661 | Test Acc: 88.52%\n",
      "Epoch 124 | Current lr: 0.0316 | Loss: 0.6645 | Test Acc: 87.99%\n",
      "Epoch 125 | Current lr: 0.0309 | Loss: 0.6619 | Test Acc: 90.32%\n",
      "Epoch 126 | Current lr: 0.0301 | Loss: 0.6601 | Test Acc: 88.77%\n",
      "Epoch 127 | Current lr: 0.0294 | Loss: 0.6606 | Test Acc: 89.79%\n",
      "Epoch 128 | Current lr: 0.0287 | Loss: 0.6575 | Test Acc: 89.81%\n",
      "Epoch 129 | Current lr: 0.0280 | Loss: 0.6590 | Test Acc: 89.25%\n",
      "Epoch 130 | Current lr: 0.0273 | Loss: 0.6566 | Test Acc: 88.50%\n",
      "Epoch 131 | Current lr: 0.0266 | Loss: 0.6555 | Test Acc: 90.62%\n",
      "Epoch 132 | Current lr: 0.0259 | Loss: 0.6540 | Test Acc: 90.18%\n",
      "Epoch 133 | Current lr: 0.0252 | Loss: 0.6533 | Test Acc: 87.09%\n",
      "Epoch 134 | Current lr: 0.0245 | Loss: 0.6522 | Test Acc: 87.86%\n",
      "Epoch 135 | Current lr: 0.0239 | Loss: 0.6502 | Test Acc: 90.49%\n",
      "Epoch 136 | Current lr: 0.0232 | Loss: 0.6467 | Test Acc: 89.18%\n",
      "Epoch 137 | Current lr: 0.0225 | Loss: 0.6440 | Test Acc: 89.06%\n",
      "Epoch 138 | Current lr: 0.0219 | Loss: 0.6453 | Test Acc: 89.41%\n",
      "Epoch 139 | Current lr: 0.0212 | Loss: 0.6457 | Test Acc: 89.90%\n",
      "Epoch 140 | Current lr: 0.0206 | Loss: 0.6419 | Test Acc: 89.70%\n",
      "Epoch 141 | Current lr: 0.0200 | Loss: 0.6417 | Test Acc: 89.92%\n",
      "Epoch 142 | Current lr: 0.0194 | Loss: 0.6412 | Test Acc: 90.25%\n",
      "Epoch 143 | Current lr: 0.0187 | Loss: 0.6412 | Test Acc: 91.47%\n",
      "Epoch 144 | Current lr: 0.0181 | Loss: 0.6369 | Test Acc: 90.53%\n",
      "Epoch 145 | Current lr: 0.0175 | Loss: 0.6372 | Test Acc: 91.21%\n",
      "Epoch 146 | Current lr: 0.0169 | Loss: 0.6364 | Test Acc: 90.34%\n",
      "Epoch 147 | Current lr: 0.0163 | Loss: 0.6332 | Test Acc: 90.90%\n",
      "Epoch 148 | Current lr: 0.0158 | Loss: 0.6339 | Test Acc: 91.07%\n",
      "Epoch 149 | Current lr: 0.0152 | Loss: 0.6331 | Test Acc: 92.03%\n",
      "Epoch 150 | Current lr: 0.0146 | Loss: 0.6306 | Test Acc: 91.94%\n",
      "Epoch 151 | Current lr: 0.0141 | Loss: 0.6285 | Test Acc: 91.85%\n",
      "Epoch 152 | Current lr: 0.0136 | Loss: 0.6296 | Test Acc: 91.61%\n",
      "Epoch 153 | Current lr: 0.0130 | Loss: 0.6282 | Test Acc: 91.36%\n",
      "Epoch 154 | Current lr: 0.0125 | Loss: 0.6259 | Test Acc: 91.85%\n",
      "Epoch 155 | Current lr: 0.0120 | Loss: 0.6242 | Test Acc: 92.34%\n",
      "Epoch 156 | Current lr: 0.0115 | Loss: 0.6231 | Test Acc: 92.41%\n",
      "Epoch 157 | Current lr: 0.0110 | Loss: 0.6217 | Test Acc: 92.54%\n",
      "Epoch 158 | Current lr: 0.0105 | Loss: 0.6219 | Test Acc: 91.52%\n",
      "Epoch 159 | Current lr: 0.0100 | Loss: 0.6204 | Test Acc: 92.14%\n",
      "Epoch 160 | Current lr: 0.0095 | Loss: 0.6183 | Test Acc: 92.35%\n",
      "Epoch 161 | Current lr: 0.0091 | Loss: 0.6173 | Test Acc: 92.48%\n",
      "Epoch 162 | Current lr: 0.0086 | Loss: 0.6179 | Test Acc: 92.54%\n",
      "Epoch 163 | Current lr: 0.0082 | Loss: 0.6159 | Test Acc: 92.86%\n",
      "Epoch 164 | Current lr: 0.0078 | Loss: 0.6160 | Test Acc: 92.62%\n",
      "Epoch 165 | Current lr: 0.0074 | Loss: 0.6140 | Test Acc: 93.42%\n",
      "Epoch 166 | Current lr: 0.0070 | Loss: 0.6135 | Test Acc: 93.03%\n",
      "Epoch 167 | Current lr: 0.0066 | Loss: 0.6132 | Test Acc: 93.11%\n",
      "Epoch 168 | Current lr: 0.0062 | Loss: 0.6116 | Test Acc: 93.87%\n",
      "Epoch 169 | Current lr: 0.0058 | Loss: 0.6105 | Test Acc: 93.63%\n",
      "Epoch 170 | Current lr: 0.0054 | Loss: 0.6106 | Test Acc: 93.77%\n",
      "Epoch 171 | Current lr: 0.0051 | Loss: 0.6102 | Test Acc: 93.72%\n",
      "Epoch 172 | Current lr: 0.0048 | Loss: 0.6101 | Test Acc: 93.56%\n",
      "Epoch 173 | Current lr: 0.0044 | Loss: 0.6098 | Test Acc: 93.82%\n",
      "Epoch 174 | Current lr: 0.0041 | Loss: 0.6094 | Test Acc: 94.17%\n",
      "Epoch 175 | Current lr: 0.0038 | Loss: 0.6091 | Test Acc: 93.50%\n",
      "Epoch 176 | Current lr: 0.0035 | Loss: 0.6090 | Test Acc: 94.14%\n",
      "Epoch 177 | Current lr: 0.0032 | Loss: 0.6091 | Test Acc: 93.92%\n",
      "Epoch 178 | Current lr: 0.0030 | Loss: 0.6090 | Test Acc: 94.01%\n",
      "Epoch 179 | Current lr: 0.0027 | Loss: 0.6086 | Test Acc: 94.23%\n",
      "Epoch 180 | Current lr: 0.0024 | Loss: 0.6086 | Test Acc: 94.15%\n",
      "Epoch 181 | Current lr: 0.0022 | Loss: 0.6085 | Test Acc: 94.11%\n",
      "Epoch 182 | Current lr: 0.0020 | Loss: 0.6083 | Test Acc: 94.21%\n",
      "Epoch 183 | Current lr: 0.0018 | Loss: 0.6082 | Test Acc: 94.14%\n",
      "Epoch 184 | Current lr: 0.0016 | Loss: 0.6078 | Test Acc: 94.22%\n",
      "Epoch 185 | Current lr: 0.0014 | Loss: 0.6079 | Test Acc: 94.20%\n",
      "Epoch 186 | Current lr: 0.0012 | Loss: 0.6082 | Test Acc: 94.28%\n",
      "Epoch 187 | Current lr: 0.0010 | Loss: 0.6078 | Test Acc: 94.20%\n",
      "Epoch 188 | Current lr: 0.0009 | Loss: 0.6083 | Test Acc: 94.24%\n",
      "Epoch 189 | Current lr: 0.0007 | Loss: 0.6082 | Test Acc: 94.21%\n",
      "Epoch 190 | Current lr: 0.0006 | Loss: 0.6077 | Test Acc: 94.18%\n",
      "Epoch 191 | Current lr: 0.0005 | Loss: 0.6082 | Test Acc: 94.21%\n",
      "Epoch 192 | Current lr: 0.0004 | Loss: 0.6081 | Test Acc: 94.25%\n",
      "Epoch 193 | Current lr: 0.0003 | Loss: 0.6077 | Test Acc: 94.14%\n",
      "Epoch 194 | Current lr: 0.0002 | Loss: 0.6081 | Test Acc: 94.19%\n",
      "Epoch 195 | Current lr: 0.0002 | Loss: 0.6083 | Test Acc: 94.18%\n",
      "Epoch 196 | Current lr: 0.0001 | Loss: 0.6076 | Test Acc: 94.21%\n",
      "Epoch 197 | Current lr: 0.0001 | Loss: 0.6083 | Test Acc: 94.20%\n",
      "Epoch 198 | Current lr: 0.0000 | Loss: 0.6079 | Test Acc: 94.19%\n",
      "Epoch 199 | Current lr: 0.0000 | Loss: 0.6079 | Test Acc: 94.23%\n",
      "Epoch 200 | Current lr: 0.0000 | Loss: 0.6077 | Test Acc: 94.19%\n",
      "\n",
      "Total training time: 7986.54 seconds\n"
     ]
    }
   ]
  }
 ]
}
