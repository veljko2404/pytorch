{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LzDRj4i1Sim6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "    transforms.RandomErasing(p=0.5)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "def get_resnet18():\n",
        "    model = resnet18(weights=None)\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    model.fc = nn.Linear(512, 10)\n",
        "    return model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg58F4USU31S",
        "outputId": "dd2d7c9b-e215-4fa9-e2f0-95161f57feba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 12.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "epochs = 200\n",
        "model = get_resnet18()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}] | Current lr: {scheduler.get_last_lr()[0]:.4f} | Loss: {running_loss/len(trainloader):.3f} | Acc: {accuracy:.2f}%\")\n",
        "\n",
        "torch.save(model.state_dict(), \"saved_models/regular_resnet18.pt\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal training time: {total_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaFkbjtgVtv-",
        "outputId": "9847a617-c571-4757-e9e9-ca9bf4e58c69"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1] | Current lr: 0.1000 | Loss: 2.221 | Acc: 30.69%\n",
            "Epoch [2] | Current lr: 0.1000 | Loss: 1.838 | Acc: 44.01%\n",
            "Epoch [3] | Current lr: 0.0999 | Loss: 1.616 | Acc: 43.40%\n",
            "Epoch [4] | Current lr: 0.0999 | Loss: 1.417 | Acc: 51.13%\n",
            "Epoch [5] | Current lr: 0.0998 | Loss: 1.270 | Acc: 64.35%\n",
            "Epoch [6] | Current lr: 0.0998 | Loss: 1.144 | Acc: 70.49%\n",
            "Epoch [7] | Current lr: 0.0997 | Loss: 1.053 | Acc: 71.89%\n",
            "Epoch [8] | Current lr: 0.0996 | Loss: 0.983 | Acc: 73.95%\n",
            "Epoch [9] | Current lr: 0.0995 | Loss: 0.939 | Acc: 75.04%\n",
            "Epoch [10] | Current lr: 0.0994 | Loss: 0.897 | Acc: 75.21%\n",
            "Epoch [11] | Current lr: 0.0993 | Loss: 0.879 | Acc: 72.02%\n",
            "Epoch [12] | Current lr: 0.0991 | Loss: 0.844 | Acc: 73.83%\n",
            "Epoch [13] | Current lr: 0.0990 | Loss: 0.825 | Acc: 79.32%\n",
            "Epoch [14] | Current lr: 0.0988 | Loss: 0.799 | Acc: 74.95%\n",
            "Epoch [15] | Current lr: 0.0986 | Loss: 0.790 | Acc: 74.61%\n",
            "Epoch [16] | Current lr: 0.0984 | Loss: 0.779 | Acc: 77.27%\n",
            "Epoch [17] | Current lr: 0.0982 | Loss: 0.767 | Acc: 79.36%\n",
            "Epoch [18] | Current lr: 0.0980 | Loss: 0.762 | Acc: 79.78%\n",
            "Epoch [19] | Current lr: 0.0978 | Loss: 0.746 | Acc: 81.78%\n",
            "Epoch [20] | Current lr: 0.0976 | Loss: 0.737 | Acc: 82.22%\n",
            "Epoch [21] | Current lr: 0.0973 | Loss: 0.733 | Acc: 79.56%\n",
            "Epoch [22] | Current lr: 0.0970 | Loss: 0.724 | Acc: 76.13%\n",
            "Epoch [23] | Current lr: 0.0968 | Loss: 0.719 | Acc: 84.32%\n",
            "Epoch [24] | Current lr: 0.0965 | Loss: 0.709 | Acc: 80.84%\n",
            "Epoch [25] | Current lr: 0.0962 | Loss: 0.708 | Acc: 76.86%\n",
            "Epoch [26] | Current lr: 0.0959 | Loss: 0.703 | Acc: 77.71%\n",
            "Epoch [27] | Current lr: 0.0956 | Loss: 0.696 | Acc: 83.43%\n",
            "Epoch [28] | Current lr: 0.0952 | Loss: 0.694 | Acc: 81.32%\n",
            "Epoch [29] | Current lr: 0.0949 | Loss: 0.685 | Acc: 80.93%\n",
            "Epoch [30] | Current lr: 0.0946 | Loss: 0.682 | Acc: 82.52%\n",
            "Epoch [31] | Current lr: 0.0942 | Loss: 0.679 | Acc: 72.61%\n",
            "Epoch [32] | Current lr: 0.0938 | Loss: 0.674 | Acc: 83.31%\n",
            "Epoch [33] | Current lr: 0.0934 | Loss: 0.671 | Acc: 80.57%\n",
            "Epoch [34] | Current lr: 0.0930 | Loss: 0.669 | Acc: 83.26%\n",
            "Epoch [35] | Current lr: 0.0926 | Loss: 0.666 | Acc: 81.72%\n",
            "Epoch [36] | Current lr: 0.0922 | Loss: 0.660 | Acc: 83.75%\n",
            "Epoch [37] | Current lr: 0.0918 | Loss: 0.664 | Acc: 77.54%\n",
            "Epoch [38] | Current lr: 0.0914 | Loss: 0.656 | Acc: 81.89%\n",
            "Epoch [39] | Current lr: 0.0909 | Loss: 0.660 | Acc: 83.67%\n",
            "Epoch [40] | Current lr: 0.0905 | Loss: 0.647 | Acc: 86.01%\n",
            "Epoch [41] | Current lr: 0.0900 | Loss: 0.645 | Acc: 76.28%\n",
            "Epoch [42] | Current lr: 0.0895 | Loss: 0.647 | Acc: 83.35%\n",
            "Epoch [43] | Current lr: 0.0890 | Loss: 0.638 | Acc: 84.30%\n",
            "Epoch [44] | Current lr: 0.0885 | Loss: 0.639 | Acc: 84.03%\n",
            "Epoch [45] | Current lr: 0.0880 | Loss: 0.635 | Acc: 86.70%\n",
            "Epoch [46] | Current lr: 0.0875 | Loss: 0.637 | Acc: 82.13%\n",
            "Epoch [47] | Current lr: 0.0870 | Loss: 0.629 | Acc: 85.36%\n",
            "Epoch [48] | Current lr: 0.0864 | Loss: 0.638 | Acc: 84.04%\n",
            "Epoch [49] | Current lr: 0.0859 | Loss: 0.621 | Acc: 86.26%\n",
            "Epoch [50] | Current lr: 0.0854 | Loss: 0.622 | Acc: 77.97%\n",
            "Epoch [51] | Current lr: 0.0848 | Loss: 0.621 | Acc: 85.77%\n",
            "Epoch [52] | Current lr: 0.0842 | Loss: 0.622 | Acc: 83.64%\n",
            "Epoch [53] | Current lr: 0.0837 | Loss: 0.617 | Acc: 86.81%\n",
            "Epoch [54] | Current lr: 0.0831 | Loss: 0.620 | Acc: 83.80%\n",
            "Epoch [55] | Current lr: 0.0825 | Loss: 0.610 | Acc: 85.56%\n",
            "Epoch [56] | Current lr: 0.0819 | Loss: 0.612 | Acc: 85.78%\n",
            "Epoch [57] | Current lr: 0.0813 | Loss: 0.607 | Acc: 82.81%\n",
            "Epoch [58] | Current lr: 0.0806 | Loss: 0.598 | Acc: 85.43%\n",
            "Epoch [59] | Current lr: 0.0800 | Loss: 0.612 | Acc: 84.30%\n",
            "Epoch [60] | Current lr: 0.0794 | Loss: 0.599 | Acc: 82.49%\n",
            "Epoch [61] | Current lr: 0.0788 | Loss: 0.602 | Acc: 84.07%\n",
            "Epoch [62] | Current lr: 0.0781 | Loss: 0.601 | Acc: 86.15%\n",
            "Epoch [63] | Current lr: 0.0775 | Loss: 0.599 | Acc: 85.42%\n",
            "Epoch [64] | Current lr: 0.0768 | Loss: 0.593 | Acc: 86.43%\n",
            "Epoch [65] | Current lr: 0.0761 | Loss: 0.586 | Acc: 85.75%\n",
            "Epoch [66] | Current lr: 0.0755 | Loss: 0.590 | Acc: 82.17%\n",
            "Epoch [67] | Current lr: 0.0748 | Loss: 0.582 | Acc: 85.61%\n",
            "Epoch [68] | Current lr: 0.0741 | Loss: 0.587 | Acc: 87.09%\n",
            "Epoch [69] | Current lr: 0.0734 | Loss: 0.582 | Acc: 84.00%\n",
            "Epoch [70] | Current lr: 0.0727 | Loss: 0.573 | Acc: 82.28%\n",
            "Epoch [71] | Current lr: 0.0720 | Loss: 0.579 | Acc: 85.50%\n",
            "Epoch [72] | Current lr: 0.0713 | Loss: 0.577 | Acc: 85.12%\n",
            "Epoch [73] | Current lr: 0.0706 | Loss: 0.567 | Acc: 85.34%\n",
            "Epoch [74] | Current lr: 0.0699 | Loss: 0.563 | Acc: 85.93%\n",
            "Epoch [75] | Current lr: 0.0691 | Loss: 0.559 | Acc: 86.72%\n",
            "Epoch [76] | Current lr: 0.0684 | Loss: 0.560 | Acc: 86.33%\n",
            "Epoch [77] | Current lr: 0.0677 | Loss: 0.558 | Acc: 87.12%\n",
            "Epoch [78] | Current lr: 0.0669 | Loss: 0.557 | Acc: 88.33%\n",
            "Epoch [79] | Current lr: 0.0662 | Loss: 0.557 | Acc: 87.93%\n",
            "Epoch [80] | Current lr: 0.0655 | Loss: 0.548 | Acc: 87.43%\n",
            "Epoch [81] | Current lr: 0.0647 | Loss: 0.546 | Acc: 86.54%\n",
            "Epoch [82] | Current lr: 0.0639 | Loss: 0.548 | Acc: 86.47%\n",
            "Epoch [83] | Current lr: 0.0632 | Loss: 0.540 | Acc: 85.12%\n",
            "Epoch [84] | Current lr: 0.0624 | Loss: 0.543 | Acc: 85.96%\n",
            "Epoch [85] | Current lr: 0.0617 | Loss: 0.544 | Acc: 84.13%\n",
            "Epoch [86] | Current lr: 0.0609 | Loss: 0.532 | Acc: 85.53%\n",
            "Epoch [87] | Current lr: 0.0601 | Loss: 0.537 | Acc: 86.91%\n",
            "Epoch [88] | Current lr: 0.0594 | Loss: 0.521 | Acc: 83.76%\n",
            "Epoch [89] | Current lr: 0.0586 | Loss: 0.533 | Acc: 86.75%\n",
            "Epoch [90] | Current lr: 0.0578 | Loss: 0.520 | Acc: 87.51%\n",
            "Epoch [91] | Current lr: 0.0570 | Loss: 0.522 | Acc: 87.55%\n",
            "Epoch [92] | Current lr: 0.0563 | Loss: 0.523 | Acc: 88.75%\n",
            "Epoch [93] | Current lr: 0.0555 | Loss: 0.518 | Acc: 86.22%\n",
            "Epoch [94] | Current lr: 0.0547 | Loss: 0.512 | Acc: 86.52%\n",
            "Epoch [95] | Current lr: 0.0539 | Loss: 0.505 | Acc: 89.62%\n",
            "Epoch [96] | Current lr: 0.0531 | Loss: 0.502 | Acc: 85.30%\n",
            "Epoch [97] | Current lr: 0.0524 | Loss: 0.505 | Acc: 88.23%\n",
            "Epoch [98] | Current lr: 0.0516 | Loss: 0.500 | Acc: 87.86%\n",
            "Epoch [99] | Current lr: 0.0508 | Loss: 0.494 | Acc: 87.80%\n",
            "Epoch [100] | Current lr: 0.0500 | Loss: 0.497 | Acc: 90.15%\n",
            "Epoch [101] | Current lr: 0.0492 | Loss: 0.490 | Acc: 86.46%\n",
            "Epoch [102] | Current lr: 0.0484 | Loss: 0.485 | Acc: 88.27%\n",
            "Epoch [103] | Current lr: 0.0476 | Loss: 0.482 | Acc: 88.80%\n",
            "Epoch [104] | Current lr: 0.0469 | Loss: 0.478 | Acc: 89.11%\n",
            "Epoch [105] | Current lr: 0.0461 | Loss: 0.478 | Acc: 85.58%\n",
            "Epoch [106] | Current lr: 0.0453 | Loss: 0.474 | Acc: 89.34%\n",
            "Epoch [107] | Current lr: 0.0445 | Loss: 0.465 | Acc: 89.40%\n",
            "Epoch [108] | Current lr: 0.0437 | Loss: 0.474 | Acc: 88.44%\n",
            "Epoch [109] | Current lr: 0.0430 | Loss: 0.458 | Acc: 90.14%\n",
            "Epoch [110] | Current lr: 0.0422 | Loss: 0.453 | Acc: 88.52%\n",
            "Epoch [111] | Current lr: 0.0414 | Loss: 0.457 | Acc: 90.37%\n",
            "Epoch [112] | Current lr: 0.0406 | Loss: 0.449 | Acc: 89.74%\n",
            "Epoch [113] | Current lr: 0.0399 | Loss: 0.451 | Acc: 88.64%\n",
            "Epoch [114] | Current lr: 0.0391 | Loss: 0.447 | Acc: 89.70%\n",
            "Epoch [115] | Current lr: 0.0383 | Loss: 0.438 | Acc: 89.01%\n",
            "Epoch [116] | Current lr: 0.0376 | Loss: 0.441 | Acc: 90.18%\n",
            "Epoch [117] | Current lr: 0.0368 | Loss: 0.437 | Acc: 87.77%\n",
            "Epoch [118] | Current lr: 0.0361 | Loss: 0.435 | Acc: 90.62%\n",
            "Epoch [119] | Current lr: 0.0353 | Loss: 0.427 | Acc: 88.35%\n",
            "Epoch [120] | Current lr: 0.0345 | Loss: 0.425 | Acc: 90.75%\n",
            "Epoch [121] | Current lr: 0.0338 | Loss: 0.417 | Acc: 89.83%\n",
            "Epoch [122] | Current lr: 0.0331 | Loss: 0.415 | Acc: 90.63%\n",
            "Epoch [123] | Current lr: 0.0323 | Loss: 0.411 | Acc: 89.55%\n",
            "Epoch [124] | Current lr: 0.0316 | Loss: 0.404 | Acc: 91.55%\n",
            "Epoch [125] | Current lr: 0.0309 | Loss: 0.407 | Acc: 90.37%\n",
            "Epoch [126] | Current lr: 0.0301 | Loss: 0.398 | Acc: 90.85%\n",
            "Epoch [127] | Current lr: 0.0294 | Loss: 0.401 | Acc: 92.25%\n",
            "Epoch [128] | Current lr: 0.0287 | Loss: 0.392 | Acc: 91.18%\n",
            "Epoch [129] | Current lr: 0.0280 | Loss: 0.384 | Acc: 92.49%\n",
            "Epoch [130] | Current lr: 0.0273 | Loss: 0.386 | Acc: 91.04%\n",
            "Epoch [131] | Current lr: 0.0266 | Loss: 0.381 | Acc: 90.29%\n",
            "Epoch [132] | Current lr: 0.0259 | Loss: 0.379 | Acc: 92.02%\n",
            "Epoch [133] | Current lr: 0.0252 | Loss: 0.384 | Acc: 90.95%\n",
            "Epoch [134] | Current lr: 0.0245 | Loss: 0.368 | Acc: 92.39%\n",
            "Epoch [135] | Current lr: 0.0239 | Loss: 0.362 | Acc: 91.64%\n",
            "Epoch [136] | Current lr: 0.0232 | Loss: 0.355 | Acc: 92.12%\n",
            "Epoch [137] | Current lr: 0.0225 | Loss: 0.355 | Acc: 91.37%\n",
            "Epoch [138] | Current lr: 0.0219 | Loss: 0.354 | Acc: 91.84%\n",
            "Epoch [139] | Current lr: 0.0212 | Loss: 0.349 | Acc: 91.51%\n",
            "Epoch [140] | Current lr: 0.0206 | Loss: 0.345 | Acc: 91.98%\n",
            "Epoch [141] | Current lr: 0.0200 | Loss: 0.342 | Acc: 92.33%\n",
            "Epoch [142] | Current lr: 0.0194 | Loss: 0.338 | Acc: 93.08%\n",
            "Epoch [143] | Current lr: 0.0187 | Loss: 0.327 | Acc: 91.83%\n",
            "Epoch [144] | Current lr: 0.0181 | Loss: 0.328 | Acc: 92.07%\n",
            "Epoch [145] | Current lr: 0.0175 | Loss: 0.325 | Acc: 93.07%\n",
            "Epoch [146] | Current lr: 0.0169 | Loss: 0.321 | Acc: 92.07%\n",
            "Epoch [147] | Current lr: 0.0163 | Loss: 0.315 | Acc: 93.11%\n",
            "Epoch [148] | Current lr: 0.0158 | Loss: 0.310 | Acc: 93.48%\n",
            "Epoch [149] | Current lr: 0.0152 | Loss: 0.305 | Acc: 93.20%\n",
            "Epoch [150] | Current lr: 0.0146 | Loss: 0.295 | Acc: 93.41%\n",
            "Epoch [151] | Current lr: 0.0141 | Loss: 0.289 | Acc: 93.43%\n",
            "Epoch [152] | Current lr: 0.0136 | Loss: 0.289 | Acc: 93.24%\n",
            "Epoch [153] | Current lr: 0.0130 | Loss: 0.285 | Acc: 93.98%\n",
            "Epoch [154] | Current lr: 0.0125 | Loss: 0.280 | Acc: 93.20%\n",
            "Epoch [155] | Current lr: 0.0120 | Loss: 0.273 | Acc: 93.64%\n",
            "Epoch [156] | Current lr: 0.0115 | Loss: 0.266 | Acc: 93.45%\n",
            "Epoch [157] | Current lr: 0.0110 | Loss: 0.268 | Acc: 93.49%\n",
            "Epoch [158] | Current lr: 0.0105 | Loss: 0.260 | Acc: 94.25%\n",
            "Epoch [159] | Current lr: 0.0100 | Loss: 0.254 | Acc: 94.48%\n",
            "Epoch [160] | Current lr: 0.0095 | Loss: 0.250 | Acc: 94.20%\n",
            "Epoch [161] | Current lr: 0.0091 | Loss: 0.245 | Acc: 94.44%\n",
            "Epoch [162] | Current lr: 0.0086 | Loss: 0.239 | Acc: 93.64%\n",
            "Epoch [163] | Current lr: 0.0082 | Loss: 0.238 | Acc: 94.46%\n",
            "Epoch [164] | Current lr: 0.0078 | Loss: 0.232 | Acc: 93.88%\n",
            "Epoch [165] | Current lr: 0.0074 | Loss: 0.222 | Acc: 94.80%\n",
            "Epoch [166] | Current lr: 0.0070 | Loss: 0.219 | Acc: 94.50%\n",
            "Epoch [167] | Current lr: 0.0066 | Loss: 0.214 | Acc: 94.56%\n",
            "Epoch [168] | Current lr: 0.0062 | Loss: 0.212 | Acc: 94.81%\n",
            "Epoch [169] | Current lr: 0.0058 | Loss: 0.202 | Acc: 95.14%\n",
            "Epoch [170] | Current lr: 0.0054 | Loss: 0.201 | Acc: 95.41%\n",
            "Epoch [171] | Current lr: 0.0051 | Loss: 0.196 | Acc: 95.46%\n",
            "Epoch [172] | Current lr: 0.0048 | Loss: 0.188 | Acc: 95.19%\n",
            "Epoch [173] | Current lr: 0.0044 | Loss: 0.187 | Acc: 95.23%\n",
            "Epoch [174] | Current lr: 0.0041 | Loss: 0.186 | Acc: 95.47%\n",
            "Epoch [175] | Current lr: 0.0038 | Loss: 0.178 | Acc: 95.34%\n",
            "Epoch [176] | Current lr: 0.0035 | Loss: 0.172 | Acc: 95.62%\n",
            "Epoch [177] | Current lr: 0.0032 | Loss: 0.165 | Acc: 95.55%\n",
            "Epoch [178] | Current lr: 0.0030 | Loss: 0.165 | Acc: 95.61%\n",
            "Epoch [179] | Current lr: 0.0027 | Loss: 0.164 | Acc: 95.84%\n",
            "Epoch [180] | Current lr: 0.0024 | Loss: 0.156 | Acc: 95.59%\n",
            "Epoch [181] | Current lr: 0.0022 | Loss: 0.152 | Acc: 95.77%\n",
            "Epoch [182] | Current lr: 0.0020 | Loss: 0.149 | Acc: 95.98%\n",
            "Epoch [183] | Current lr: 0.0018 | Loss: 0.147 | Acc: 95.94%\n",
            "Epoch [184] | Current lr: 0.0016 | Loss: 0.145 | Acc: 95.95%\n",
            "Epoch [185] | Current lr: 0.0014 | Loss: 0.142 | Acc: 95.97%\n",
            "Epoch [186] | Current lr: 0.0012 | Loss: 0.137 | Acc: 96.06%\n",
            "Epoch [187] | Current lr: 0.0010 | Loss: 0.140 | Acc: 96.04%\n",
            "Epoch [188] | Current lr: 0.0009 | Loss: 0.132 | Acc: 96.09%\n",
            "Epoch [189] | Current lr: 0.0007 | Loss: 0.133 | Acc: 96.25%\n",
            "Epoch [190] | Current lr: 0.0006 | Loss: 0.126 | Acc: 96.26%\n",
            "Epoch [191] | Current lr: 0.0005 | Loss: 0.128 | Acc: 96.30%\n",
            "Epoch [192] | Current lr: 0.0004 | Loss: 0.128 | Acc: 96.35%\n",
            "Epoch [193] | Current lr: 0.0003 | Loss: 0.125 | Acc: 96.24%\n",
            "Epoch [194] | Current lr: 0.0002 | Loss: 0.127 | Acc: 96.27%\n",
            "Epoch [195] | Current lr: 0.0002 | Loss: 0.125 | Acc: 96.32%\n",
            "Epoch [196] | Current lr: 0.0001 | Loss: 0.123 | Acc: 96.32%\n",
            "Epoch [197] | Current lr: 0.0001 | Loss: 0.123 | Acc: 96.28%\n",
            "Epoch [198] | Current lr: 0.0000 | Loss: 0.121 | Acc: 96.31%\n",
            "Epoch [199] | Current lr: 0.0000 | Loss: 0.125 | Acc: 96.32%\n",
            "Epoch [200] | Current lr: 0.0000 | Loss: 0.121 | Acc: 96.29%\n",
            "\n",
            "Total training time: 4697.05 seconds\n"
          ]
        }
      ]
    }
  ]
}